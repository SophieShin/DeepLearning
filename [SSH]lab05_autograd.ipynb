{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "[SSH]lab05-autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophieShin/DeepLearning/blob/main/%5BSSH%5Dlab05_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkR5FpvOqMtA"
      },
      "source": [
        "# Lab 5 – Autograd\n",
        "\n",
        "Before working on this notebook:\n",
        "  - Create a copy in your drive\n",
        "  - Set your Runtime to None\n",
        "\n",
        "Adapted from: [Original Source](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2IzMmVuNGxhUm9XcnN1UXVydWVjZEpkaEJvd3xBQ3Jtc0ttTkp6Tml1MzlqT1Fia3dFNTdteFVlbW5BVGxDNzMxZW51YzVnTUR6cURhOU1TRHdqQmtUaWZfekppMkswUm52SUZ6d285SHA5YVdfMHF3WmhyYWZuODNER0trLTUyM3VQNHpCcnEtakZxWXMwNXI1RQ&q=https%3A%2F%2Fpytorch-tutorial-assets.s3.amazonaws.com%2Fyoutube-series%2FVideo%2B3%2B-%2BAutograd.ipynb)\n",
        "\n",
        "PyTorch's *Autograd* feature is part of what make PyTorch flexible and fast for building machine learning projects. It allows for the rapid and easy computation of multiple partial derivatives (also referred to as *gradients)* over a complex computation. This operation is central to backpropagation-based neural network learning.\n",
        "\n",
        "The power of autograd comes from the fact that it traces your computation dynamically *at runtime,* meaning that if your model has decision branches, or loops whose lengths are not known until runtime, the computation will still be traced correctly, and you'll get correct gradients to drive learning. This, combined with the fact that your models are built in Python, offers far more flexibility than frameworks that rely on static analysis of a more rigidly-structured model for computing gradients.\n",
        "\n",
        "## What Do We Need Autograd For?\n",
        "\n",
        "In training a model, we want to minimize the loss, $L$. In the idealized case of a perfect model, that means adjusting its learning weights - that is, the adjustable parameters of the function - such that loss is zero for all inputs. In the real world, it means an iterative process of nudging the learning weights until we see that we get a tolerable loss for a wide variety of inputs.\n",
        "\n",
        "How do we decide how far and in which direction to nudge the weights? We want to *minimize* the loss, which means making its first derivative with respect to the input equal to 0: $\\frac{\\partial L}{\\partial x} = 0$.\n",
        "\n",
        "Recall, though, that the loss is not *directly* derived from the input, but a function of the model's output (which is a function of the input directly). By the chain rule of differential calculus, we have $\\frac{\\partial {L({\\vec y})}}{\\partial x}$ = $\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}$ = $\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}$.\n",
        "\n",
        "$\\frac{\\partial M(x)}{\\partial x}$ is where things get complex. The partial derivatives of the model's outputs with respect to its inputs, if we were to expand the expression using the chain rule again, would involve many local partial derivatives over every multiplied learning weight, every activation function, and every other mathematical transformation in the model. The full expression for each such partial derivative is the sum of the products of the local gradient of *every possible path* through the computation graph that ends with the variable whose gradient we are trying to measure.\n",
        "\n",
        "In particular, the gradients over the learning weights are of interest to us - they tell us *what direction to change each weight* to get the loss function closer to zero.\n",
        "\n",
        "Since the number of such local derivatives (each corresponding to a separate path through the model's computation graph) will tend to go up exponentially with the depth of a neural network, so does the complexity in computing them. This is where autograd comes in: It tracks the history of every computation. Every computed tensor in your PyTorch model carries a history of its input tensors and the function used to create it. Combined with the fact that PyTorch functions meant to act on tensors each have a built-in implementation for computing their own derivatives, this greatly speeds the computation of the local derivatives needed for learning.\n",
        "\n",
        "## A Simple Example\n",
        "\n",
        "Let's start with a straightforward example. First, we'll do some imports to let us graph our results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUj6dncKqMtD"
      },
      "source": [
        "%matplotlib inline\n",
        "# Jupyer notebook을 실행한 브라우저에서 그림을 볼 수  있도록,\n",
        "# 브라우저 내부(Inline)에 그려지도록 하는 코드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvvyf_HHqMtE"
      },
      "source": [
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Y-zYhLqMtF"
      },
      "source": [
        "Next, we'll create an input tensor full of evenly spaced values on the interval $[0, 2{\\pi}]$, and specify `requires_grad=True`. (Like most functions that create tensors, `torch.linspace()` accepts an optional `requires_grad` option.) Setting this flag means that in every computation that follows, autograd will be accumulating the history of the computation in the output tensors of that computation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.linspace(0., 2. * math.pi, steps = 25, requires_grad = True)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQDWpQcPSIC",
        "outputId": "38b6fd5e-5f05-4fb9-96c3-34eedbaad844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
            "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
            "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1. What would happen if requires_grad is not set to True?\n",
        "#A1. It then enables to compute the derivatives when backpropagation by storing computing history."
      ],
      "metadata": {
        "id": "Jav30TPR_xiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa= torch.linspace(0., 2. * math.pi, steps = 25, requires_grad = False)\n",
        "print(aa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEX-CPJyPbTQ",
        "outputId": "388cf72b-857f-4c6a-9868-894e5b0e6949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
            "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
            "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybsuIr62qMtG"
      },
      "source": [
        "Next, we'll perform a computation, and plot its output in terms of its inputs:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.sin(a)\n",
        "plt.plot(a.detach(), b.detach()) # can't call plot on tensors that require grads. Detach them first"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "1IoRHW5oPnb7",
        "outputId": "4101ef3e-de59-4faa-cd50-9b76f39c1b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f60d0ca7e90>]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ysTX39TqMtI"
      },
      "source": [
        "Let's have a closer look at the tensor `b`. When we print it, we see an indicator that it is tracking its computation history:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW8Qr2b6qMtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a361ac-3468-4c4d-e003-49f2379c757d"
      },
      "source": [
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,\n",
            "         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,\n",
            "         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,\n",
            "        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,\n",
            "        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],\n",
            "       grad_fn=<SinBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. What does the argument grad_fn=<SinBackward0> indicate here?\n",
        "#A2. In order to be ready for the case of taking derivative, it stores the derivates of sin function."
      ],
      "metadata": {
        "id": "lCo5M2Ew4ieZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqoyjePmqMtJ"
      },
      "source": [
        "Let's perform some more computations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VWsODeNqMtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ee13d3-1644-41d6-e207-1d475c9df5b5"
      },
      "source": [
        "c = 2 * b\n",
        "print(f'c = {c}')\n",
        "\n",
        "d = c + 1\n",
        "print(f'd = {d}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c = tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
            "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
            "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
            "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
            "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
            "       grad_fn=<MulBackward0>)\n",
            "d = tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
            "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
            "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
            "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
            "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AbHP-GTqMtK"
      },
      "source": [
        "Finally, let's compute a single-element output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6pJkaxGqMtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8531c472-dbf3-43f4-ed58-7a038b50a9e0"
      },
      "source": [
        "out = d.sum()\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(25., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geLHAAMwqMtK"
      },
      "source": [
        "## Tracking the computations\n",
        "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its `next_functions` property. We can see below that drilling down on this property on `d` shows us the gradient functions for all the prior tensors. Note that `a.grad_fn` is reported as `None`, indicating that this was an input to the function with no history of its own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crvLA4mQqMtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb61f16-b8e1-41ff-92e2-8662e7d6a84b"
      },
      "source": [
        "print('d:')\n",
        "print(d.grad_fn)\n",
        "print(d.grad_fn.next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print('\\nc:')\n",
        "print(c.grad_fn)\n",
        "print('\\nb:')\n",
        "print(b.grad_fn)\n",
        "print('\\na:')\n",
        "print(a.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d:\n",
            "<AddBackward0 object at 0x7f60d078b810>\n",
            "((<MulBackward0 object at 0x7f60d078b9d0>, 0), (None, 0))\n",
            "((<SinBackward0 object at 0x7f60d078b890>, 0), (None, 0))\n",
            "((<AccumulateGrad object at 0x7f60d078b8d0>, 0),)\n",
            "()\n",
            "\n",
            "c:\n",
            "<MulBackward0 object at 0x7f60d078b9d0>\n",
            "\n",
            "b:\n",
            "<SinBackward0 object at 0x7f60d078b9d0>\n",
            "\n",
            "a:\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-omojMIqMtL"
      },
      "source": [
        "With all this machinery in place, how do we get derivatives out? You call the `backward()` method on the output, and check the input's `grad` property to inspect the gradients. When you call `.backward()` on a tensor with no arguments, it expects the calling tensor to contain only a single element, as is the case when computing a loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWRM19ZpqMtL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8534404b-46b7-439a-fee8-6376cafb9f4e"
      },
      "source": [
        "out.backward()\n",
        "print(a.grad)\n",
        "plt.plot(a.detach(), a.grad.detach())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
            "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
            "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
            "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
            "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f60d07ad9d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(d.grad)\n",
        "# except a.grad, b/c/d.grad provide 'none'. Q. why??\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyPDoWyQAOQB",
        "outputId": "b01da993-ae97-465b-adb8-987738d6169c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
            "  return self._grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. What is the range of values of the gradients for this function?\n",
        "#A3. It ranges from -2 to 2."
      ],
      "metadata": {
        "id": "i6D10PWr7EA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70UPEyxDqMtL"
      },
      "source": [
        "Recall the computation steps we took to get here:\n",
        "\n",
        "```\n",
        "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "b = torch.sin(a)\n",
        "c = 2 * b\n",
        "d = c + 1\n",
        "out = d.sum()\n",
        "```\n",
        "\n",
        "Adding a constant, as we did to compute `d`, does not change the derivative. That leaves $c = 2 * b = 2 * sin(a)$, the derivative of which should be $2 * cos(a)$. Looking at the graph above, that's just what we see.\n",
        "\n",
        "### Exercise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Create a tensor containing the numbers 0 to 4 (5 floats) and set it to keep track of the history of the computation in the output tensors\n",
        "x = torch.linspace(0., 4., steps = 5, requires_grad = True)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "GJKH2mTIBICE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c4beab-b8b1-4d1e-c84d-a407798f6c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3., 4.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x*x\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvW3GQBmZklD",
        "outputId": "ac94294f-e884-44cb-bb48-0b95032e140e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  1.,  4.,  9., 16.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "ejnoHHDrlGzn",
        "outputId": "b67f131d-3202-476b-ebb2-f2e01c10f8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ab75bb780f4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x.zero_grad()\n",
        "\n",
        "# Q5. Create a new tensor y to be the dot product of x on itself. Use torch.dot()\n",
        "y = torch.dot(x, x)\n",
        "\n",
        "#print(y)\n",
        "print(y)\n",
        "\n",
        "# Q6. Calculate the gradient of y with respect to x by calling the function for backpropagation \n",
        "y.backward()\n",
        "\n",
        "# Q7. Print the gradient\n",
        "print(x.grad)\n"
      ],
      "metadata": {
        "id": "RdBlQCwbDF-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff78d0e-ced0-4b70-ce46-1bef28d642dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(30., grad_fn=<DotBackward0>)\n",
            "tensor([0., 2., 4., 6., 8.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Are the gradients consistent with what you would expect from differentiating the function y = x^2?\n",
        "# A8. Yes, it is 2*x\n",
        "\n",
        "# Q9. Plot the graph for x and its gradients. Use detach() to detach them first\n",
        "plt.plot(x.detach(), x.grad.detach())"
      ],
      "metadata": {
        "id": "OXZrc9_6Extn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6f45c2ba-6b2e-4c8a-e151-fb9d3c162ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f60cfecaa10>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3G8c+PLECAsIadkGDYSRAIIOCKogKiIHrd6ka5WNve6m0rICqioIJa17qBe3GpJWERBZGiiBsKCNlIIOyBQMKWhISQZX73j6ReSkEmMDNnZvK8X6+8nDCHmceTnIeTk3O+x1hrERER/1XH6QAiIvLLVNQiIn5ORS0i4udU1CIifk5FLSLi50K98aItWrSwMTEx3nhpEZGgtHbt2v3W2qiTPeeVoo6JiWHNmjXeeGkRkaBkjNlxqud06ENExM+pqEVE/JyKWkTEz6moRUT8nIpaRMTPuVXUxpj/NcakG2PSjDEfGGPqeTuYiIhUOW1RG2PaAX8AEq21vYAQ4EZvBxMRkSruHvoIBeobY0KBCGCP9yKJiASeH7cf5NWVW7zy2qctamvtbuBpYCeQCxRYa5eduJwxZoIxZo0xZk1+fr7nk4qI+KEjxyqYujCN61/9jvdX76SkrMLj7+HOoY+mwDVALNAWaGCM+dWJy1lrZ1trE621iVFRJ70KUkQkqHyZlccVz37F377fwZ1DYlhyzwVEhHv+gm93XvEyYJu1Nh/AGJMMDAbmejyNiEgAOFRcxvRPMkhet5u4lg2Z95vB9OvY1Gvv505R7wTOM8ZEAEeBSwEN8hCRWsday5K0vUxdmMbhknL+Z2gcvx8aR93QEK++72mL2lq72hgzD1gHVAA/AbO9mkpExM/kFZby0MI0PkvfR3y7xrw7biA92kb65L3dOphirX0YeNjLWURE/I61ln+szWHG4gyOVbiYPLwb48+PJTTEd9cLemXMqYhIMNh1sIT7k1P5Ons/A2KbMfPaeDpFNfR5DhW1iMgJKl2Wd77dzlOfZRFSxzBjdC9uHhBNnTrGkTwqahGR42zeV8SkpBTW7TzMxV2jeHxMPG2b1Hc0k4paRAQor3Tx6pdbeHFFNg3qhvDcDedyzbltMcaZvejjqahFpNZLzSngvnkbyNxbxKjebXl4VA9aNKzrdKyfqahFpNYqLa/k2eWbmPPVVqIa1WXObYkM69HK6Vj/QUUtIrXS91sPMDkphe0HSrhpQAcmD+9O4/phTsc6KRW1iNQqRaXlzFySyXurdxLdLIL3xw9kcFwLp2P9IhW1iNQaX2TmMWV+KvsKSxl/fix/vLyLV4YoeZr/JxQROUsHi8t49ON0FqzfQ5dWDXn5lsH0ifbeECVPU1GLSNCy1rI4JZdpi9IpLC3nnks787tL4ggPDazbxaqoRSQo7S0o5cEFaSzfuI/e7Rsz67qBdGvtmyFKnqaiFpGgYq3lwx938fgnGyl3uXhgRHfGnR9LiEOXf3uCilpEgsaOA8VMTkrlu60HOK9TM2Zem0BMiwZOxzprKmoRCXiVLstb32zj6WVZhNWpwxPXxnNDYgfHhih5mopaRAJa1t4iJialsGHXYS7r3pIZo+Np3bie07E86rRFbYzpCvz9uD/qBEy11j7ntVQiIqdRVuHi5S+zeemLbBrVC+OFm/owKqGNXwxR8jR3bsWVBZwLYIwJAXYD872cS0TklNbvOsykeSlk7SvimnPb8vConjRrEO50LK+p6aGPS4Et1tod3ggjIvJLjpZV8sznWbzx9TZaNqrHG7cncml3/xui5Gk1LeobgQ9O9oQxZgIwASA6OvosY4mI/Ltvt+xnclIqOw+WcMvAaCYN70ZkPf8couRpxlrr3oLGhAN7gJ7W2n2/tGxiYqJds2aNB+KJSG1XWFrOE59m8sEPO4lpHsHMsQmc16m507E8zhiz1lqbeLLnarJHPRxYd7qSFhHxlOUZ+3hgQSr5Rce468JO3HtZF+qHhzgdy+dqUtQ3cYrDHiIinnTgyDGmfZzBxxv20K11I+bclkhC+yZOx3KMW0VtjGkADAPu8m4cEanNrLUs2rCHaYvSOXKsgj8O68JvLjon4IYoeZpbRW2tLQaC76CQiPiNPYeP8uCCNFZk5tEnugmzxibQpVUjp2P5BV2ZKCKOcrks7/+wk5lLMql0WaZe1YPbB8cE9BAlT1NRi4hjtu0vZnJSCqu3HWRIXHOeGJNAdPMIp2P5HRW1iPhcRaWLN77exjOfbyI8tA5Pjk3g+sT2QXn5tyeoqEXEpzbmFjIpKYWUnAKG9WjFjNG9aBUZXEOUPE1FLSI+cayikpdWZPPyl1toEhHGSzf3ZUR8a+1Fu0FFLSJet3bHISYlpZCdd4Rr+7bjoZE9aBrEQ5Q8TUUtIl5TUlbBU59l8fa322kTWY+37uzPJV1bOh0r4KioRcQrvt68n8nJKeQcOsptgzoy8cpuNKyryjkTWmsi4lEFR8t57JMMPlqTQ2yLBnx01yAGxDZzOlZAU1GLiMd8lr6XhxakcaC4jLsvPod7Lu1MvbDaN0TJ01TUInLW8ouOMW1ROp+k5tKjTSRv3tGfXu0aOx0raKioReSMWWtJXrebRxdncLSskvuu6MqECzsRFlK7hyh5mopaRM7I7sNHmZKcyspN+fTr2JRZYxOIa9nQ6VhBSUUtIjXiclnmrt7BrCWZWOCRq3ty63kdqaMhSl6johYRt23JP8LkpBR+3H6ICzq34PEx8XRopiFK3qaiFpHTKq90MWfVVp5bvpn6YSE8fX1vxvZtp8u/fcTdO7w0AV4HegEWGGet/c6bwUTEP6TtLmBSUgrpewoZ3qs1j1zTk5aNNETJl9zdo34eWGqtva76buT6WUckyJWWV/Liis28unIrTSPCeeWWvgyPb+N0rFrptEVtjGkMXAjcAWCtLQPKvBtLRJy0ZvtBJiWlsCW/mOv7teeBkd1pEqEhSk5xZ486FsgH3jLG9AbWAvdU30fxZ8aYCcAEgOjoaE/nFBEfKD5WNUTpne+207Zxfd4dN4ALu0Q5HavWc+es9FCgL/CKtbYPUAxMPnEha+1sa22itTYxKkpfWJFAs3JTPpc/+xXvfLed2wfFsOx/L1RJ+wl39qhzgBxr7erqz+dxkqIWkcB0uKSM6Ys3krQuh3OiGvCPuwaRGKMhSv7ktEVtrd1rjNlljOlqrc0CLgUyvB9NRLxtSWouDy1M51BJGb+/JI7fD43TECU/5O5ZH/8DvFd9xsdW4E7vRRIRb8srLGXqwnSWpu+lZ9tI3hnXn55tNUTJX7lV1Nba9UCil7OIiJdZa5m3NofpizMorXAx6cpu/PcFsYRqiJJf05WJIrXEroMlTJmfyqrN+xkQ04wnxsZzTpSGKAUCFbVIkKt0Wd79bjtPfZaFAaZf05NbBmqIUiBRUYsEsey8IiYlpbJ2xyEu6hLF49fG065JfadjSQ2pqEWCUHmli9dWbuGFf2YTUTeEZ/6rN2P6aIhSoFJRiwSZtN0F3DcvhY25hYxMaMO0UT2JalTX6VhyFlTUIkGitLyS55ZvZs6qrTRrEM5rt/bjip6tnY4lHqCiFgkCP2w7yOSkFLbuL+aGxA5MGdGdxhFhTscSD1FRiwSwotJynlyaxd++30H7pvWZ++uBnN+5hdOxxMNU1CIB6ousPB5ITiW3sJRxQ2L58xVdiAjXJh2M9FUVCTCHisuYvjiD5J9207llQ5LuHkzf6KZOxxIvUlGLBAhrLZ+k5vLwwnQKjpbzh6Fx/G5oHHVDNUQp2KmoRQLAvsJSHlqQxrKMfcS3a8zc8QPp3ibS6VjiIypqET9mreWjNbuY8clGyipc3D+8G78+X0OUahsVtYif2nmghMnJKXy75QADYpsxa2wCsS0aOB1LHKCiFvEzlS7L299u5+nPsgipY5gxuhc3D4jWEKVazK2iNsZsB4qASqDCWqvZ1CJesGlfERPnpbB+12GGdmvJjNG9aKshSrVeTfaoL7HW7vdaEpFarKzCxasrt/Diis00rBvK8zeey9W922qIkgA69CHiuA27DjMpKYXMvUWM6t2WaaN60LyhhijJ/3O3qC2wzBhjgdestbNPXMAYMwGYABAdHe25hCJB6mhZJc8u38Trq7YS1aguc25LZFiPVk7HEj/kblGfb63dbYxpCXxujMm01n51/ALV5T0bIDEx0Xo4p0hQ+W7LAe5PTmH7gRJuGtCB+0d0J7KehijJybl7c9vd1f/NM8bMBwYAX/3y3xKRExWWljNzSSbvr95JdLMI3h8/kMFxGqIkv+y0RW2MaQDUsdYWVT++HHjU68lEgsyKzH1MSU4jr6iU8efH8qfLu1I/XJd/y+m5s0fdCphf/dvnUOB9a+1Sr6YSCSIHjhzj0cUZLFy/hy6tGvLKrwbTR0OUpAZOW9TW2q1Abx9kEQkq1lo+Tsll2qJ0ikrLufeyzvz24jjCQ3X5t9SMTs8T8YLcgqM8tCCN5Rvz6N2hCU+OTaBr60ZOx5IApaIW8SCXy/Lhj7t44tONlLtcPDiyO3cOiSVEl3/LWVBRi3jI9v3FTE5O4futBxnUqTkzx8bTsbmGKMnZU1GLnKVKl+XNr7fxl8+zCKtThyeujefG/h10+bd4jIpa5Cxk7S1i4rwNbMgp4LLuLZkxOp7Wjes5HUuCjIpa5AyUVbh46YtsXv4ym0b1wnjhpj6MSmijvWjxChW1SA39tPMQk5JS2LTvCKPPbcvUUT1p1iDc6VgSxFTUIm4qKavgL8s28eY322gdWY8370hkaDcNURLvU1GLuOHb7P1MTk5l58ESbhkYzeTh3WikIUriIypqkV9QcLScJz7dyIc/7iKmeQQfTjiP8zo1dzqW1DIqapFT+DxjHw8uSCW/6Bh3XdiJey/roiFK4ggVtcgJ9h85xrRF6SxOyaVb60bMuS2RhPZNnI4ltZiKWqSatZaF6/fwyMfpHDlWwR+HdeE3F52jIUriOBW1CLDn8FEemJ/KF1n59IluwqyxCXRppSFK4h9U1FKruVyW937YyawlmVS6LFOv6sHtg2M0REn8iopaaq1t+4uZlJTCD9sOMiSuOU+MSSC6eYTTsUT+g9tFbYwJAdYAu621V3kvkoh3VVS6eP3rbTz7+SbCQ+vw5NgErk9sr8u/xW/VZI/6HmAjEOmlLCJel7GnkElJKaTuLmBYj1bMGN2LVpEaoiT+za2iNsa0B0YCjwF/9GoiES84VlHJX1dk88qXW2gSEcZLN/dlRHxr7UVLQHB3j/o5YCJwyl+DG2MmABMAoqOjzz6ZiIes3VE1RCk77whj+rRj6lU9aKohShJATlvUxpirgDxr7VpjzMWnWs5aOxuYDZCYmGg9llDkDBUfq+DpZVm8/e122kTW4607+3NJ15ZOxxKpMXf2qIcAVxtjRgD1gEhjzFxr7a+8G03kzK3anM/9yankHDrKbYM6MvHKbjSsq5OcJDCd9jvXWns/cD9A9R71n1XS4q8KSsp57NMMPlqTQ2yLBnx01yAGxDZzOpbIWdEuhgSNpWl7eWhhGgeLy7j74nO459LO1AvTECUJfDUqamvtl8CXXkkicobyi6qGKH2Smkv3NpG8eXt/4ts3djqWiMdoj1oClrWW5HW7eXRxBkfLKrnviq5MuLATYSEaoiTBRUUtASnnUAlT5qfx1aZ8+nVsyqyx8cS11BAlCU4qagkoLpdl7uodzFqSiQWmjerBbYNiqKMhShLEVNQSMLbkH2FyUgo/bj/EBZ1b8PiYeDo00xAlCX4qavF75ZUu5qzaynPLN1MvtA5PXZfAdf00RElqDxW1+LW03QVMSkohfU8hV/ZszaOje9KykYYoSe2ioha/VFpeyYsrNvPqyq00jQjnlVv6Mjy+jdOxRByhoha/s2b7QSYmpbA1v5jr+rXnwZHdaRKhIUpSe6moxW8cOVbBU0szeff7HbRtXJ93xw3gwi5RTscScZyKWvzCyk35TElOZU/BUW4fFMN9V3SlgYYoiQAqanHY4ZIypi/eSNK6HDpFNeAfdw0iMUZDlESOp6IWxyxJzeWhhekcKinjd5ecw/8M1RAlkZNRUYvP5RWWMnVhOkvT99KzbSTvjOtPz7YaoiRyKipq8RlrLfPW5jB9cQalFS4mXdmN8RfEaoiSyGmoqMUndh0sYcr8VFZt3k//mKbMHJvAOVENnY4lEhBU1OJVlS7L377bzpOfZWGA6df05JaBHTVESaQG3Lm5bT3gK6Bu9fLzrLUPezuYBL7svCImJaWydschLuoSxWNjetG+qYYoidSUO3vUx4Ch1tojxpgw4GtjzBJr7fdeziYBqrzSxWsrt/DCP7OJqBvCM//VmzF92mmIksgZcufmthY4Uv1pWPWH9WYoCVxpuwu4b14KG3MLGRnfhmlX9ySqUV2nY4kENLeOURtjQoC1QBzwkrV29UmWmQBMAIiOjvZkRgkApeWVPLd8M3NWbaVZg3Be/VU/ruzV2ulYIkHBraK21lYC5xpjmgDzjTG9rLVpJywzG5gNkJiYqD3uWuSHbQeZnJTC1v3F3JDYgSkjutM4IszpWCJBo6Z3IT9sjPkCuBJIO93yEtyOHKtg1pJM/vb9Dto3rc/cXw/k/M4tnI4lEnTcOesjCiivLun6wDBglteTiV/7IiuPB5JTyS0sZdyQWP58RRciwnW2p4g3uLNltQHeqT5OXQf4yFq72LuxxF8dKi5j+uIMkn/aTVzLhsz7zWD6dWzqdCyRoObOWR8pQB8fZBE/Zq3lk9RcHl6YTsHRcv4wNI7fDY2jbqiGKIl4m35WldPaV1jKQwvSWJaxj/h2jZk7fiDd20Q6HUuk1lBRyylZa/lozS5mfLKRsgoX9w/vxq/PjyVUQ5REfEpFLSe180AJ989P4ZvsAwyIbcassQnEtmjgdCyRWklFLf+m0mV5+9vtPP1ZFiF1DDNG9+LmAdEaoiTiIBW1/GzzviImJqXw087DXNI1isfGxNO2SX2nY4nUeipqoazCxasrt/Diis00rBvK8zeey9W922qIkoifUFHXcht2HWZSUgqZe4sY1bst00b1oHlDDVES8Scq6lrqaFklzy3fxJxVW4lqVJc5tyUyrEcrp2OJyEmoqGuh77ceYHJSCtsPlHDTgA7cP6I7kfU0REnEX6moa5Gi0nJmLsnkvdU7iW4WwfvjBzI4TkOURPydirqWWJG5jwfmp7GvsJTx58fyp8u7Uj9cl3+LBAIVdZA7WFzGox+ns2D9Hrq0asjLtwymT7SGKIkEEhV1kLLW8nFKLtMWpVNUWs69l3XmtxfHER6qy79FAo2KOgjtLSjlwQWpLN+YR+8OTXhybAJdWzdyOpaInCEVdRCx1vLhj7t4/JONlLtcPDiyO3cOiSVEl3+LBDR37vDSAXgXaEXV3cdnW2uf93YwqZkdB4qZnJTKd1sPMKhTc2aOjadjcw1REgkG7uxRVwB/stauM8Y0AtYaYz631mZ4OZu4odJleeubbTy9LIuwOnV44tp4buzfQZd/iwQRd+7wkgvkVj8uMsZsBNoBKmqHZe2tGqK0YddhLuvekhmj42nduJ7TsUTEw2p0jNoYE0PVbblWn+S5CcAEgOjoaA9Ek1Mpq3Dx0hfZvPxlNpH1wnjxpj5cldBGe9EiQcrtojbGNASSgHuttYUnPm+tnQ3MBkhMTLQeSyj/Zv2uw0yct4FN+44w+ty2TB3Vk2YNwp2OJSJe5FZRG2PCqCrp96y1yd6NJCdztKySvyzL4s1vttEqsh5v3pHI0G4aoiRSG7hz1ocB3gA2Wmuf8X4kOdG3W/YzOSmVnQdLuGVgNJOHd6ORhiiJ1Bru7FEPAW4FUo0x66v/bIq19lPvxRKAwtJynvh0Ix/8sIuY5hF8OOE8zuvU3OlYIuJj7pz18TWg31L52OcZ+3hwQSr5Rce468JO3HtZFw1REqmldGWin9l/5BjTFqWzOCWXbq0bMee2RBLaN3E6log4SEXtJ6y1LFy/h0c+Tqf4WCV/GtaFuy46R0OURERF7Q/2HD7KgwvSWJGZR5/oqiFKnVtpiJKIVFFRO8jlsrz/w05mLsmk0mWZelUPbh8coyFKIvJvVNQO2ba/mElJKfyw7SDnx7XgiWvj6dAswulYIuKHVNQ+VlHp4vWvt/Hs55sID63Dk2MTuD6xvS7/FpFTUlH7UMaeQiYlpZC6u4DLe7Ri+uhetIrUECUR+WUqah84VlHJX1dk88qXW2gSEcZLN/dlRHxr7UWLiFtU1F62dschJiWlkJ13hGv7tuOhkT1oqiFKIlIDKmovKT5WwdPLsnj72+20iazHW3f255KuLZ2OJSIBSEXtBas253N/cio5h45y26COTLyyGw3ralWLyJlRe3hQQUk5j32awUdrcujUogEf3TWIAbHNnI4lIgFORe0hS9P28tDCNA4Wl3H3xedwz6WdqRemIUoicvZU1Gcpv6hqiNInqbn0aBPJW3f0p1e7xk7HEpEgoqI+Q9Zaktft5tHFGRwtq+S+K7oy4cJOhIVoiJKIeJaK+gzkHCphyvw0vtqUT7+OTZk1NoG4lg2djiUiQcqdW3G9CVwF5Flre3k/kv9yuSxzV+9g1pJMLPDI1T259byO1NEQJRHxInf2qN8G/gq8690o/m1L/hEmJ6Xw4/ZDXNC5BY+P0RAlEfENd27F9ZUxJsb7UfxTeaWLOau28tzyzdQPC+Hp63sztm87Xf4tIj7jsWPUxpgJwASA6OhoT72so9J2FzApKYX0PYUM79WaR67pSctGGqIkIr7lsaK21s4GZgMkJiZaT72uE0rLK3nhn5t57autNI0I55Vb+jI8vo3TsUSkltJZHydYs/0gE5NS2JpfzPX92vPgyB40jghzOpaI1GIq6mpHjlXw1NJM3v1+B20b1+fdcQO4sEuU07FERNw6Pe8D4GKghTEmB3jYWvuGt4P50spN+UxJTmVPwVFuHxTDfVd0pYGGKImIn3DnrI+bfBHECYdLypi+eCNJ63I4J6oB/7hrEIkxGqIkIv6l1u42fpqay9SFaRwqKef3l8Tx+6FxGqIkIn6p1hV1XmEpUxemszR9L73aRfLOuAH0bKshSiLiv2pNUVtr+cfaHGYszqC0wsWkK7vx3xfEEqohSiLi52pFUe86WMKU+ams2ryfATHNmDk2nk5RGqIkIoEhqIu60mV597vtPLk0izoGpl/Tk1sGaoiSiASWoC3q7LwiJs5LYd3Ow1zUJYrHr42nXZP6TscSEamxoCvq8koXr63cwgv/zCaibgjP3tCb0edqiJKIBK6gKurUnALum7eBzL1FjExowyNX96RFw7pOxxIROStBUdSl5ZU8u3wTr6/aRvMG4bx2az+u6Nna6VgiIh4R8EW9eusBJiensm1/MTckdmDKyO40rq8hSiISPAK2qItKy5m1NJO53++kQ7P6vDd+IEPiWjgdS0TE4wKyqL/IzOOB+ankFpYybkgsf76iCxHhAfm/IiJyWgHVbgeLy5i+OIP5P+2mc8uGJN09mL7RTZ2OJSLiVQFR1NZaFqfkMm1ROgVHy/nDpZ353SXnUDdUQ5REJPj5fVHvKyzlgflpLN+4j4T2jZk7fiDd20Q6HUtExGf8tqittfz9x1089ulGyipcTBnRjXFDNERJRGoft4raGHMl8DwQArxurZ3pzVA7D5QwOTmFb7ccYGBsM2aNTSCmRQNvvqWIiN9y51ZcIcBLwDAgB/jRGLPIWpvh6TCVLstb32zj6WVZhNapw2NjenFT/2gNURKRWs2dPeoBQLa1diuAMeZD4BrAo0VdUFLO7W/9wPpdhxnarSWPjelFm8YaoiQi4k5RtwN2Hfd5DjDwxIWMMROACQDR0dE1DhJZP5SOzSO4c0gMV/duqyFKIiLVPPbLRGvtbGA2QGJioq3p3zfG8PyNfTwVR0QkaLhzCsVuoMNxn7ev/jMREfEBd4r6R6CzMSbWGBMO3Ags8m4sERH5l9Me+rDWVhhjfg98RtXpeW9aa9O9nkxERAA3j1Fbaz8FPvVyFhEROQld5ici4udU1CIifk5FLSLi51TUIiJ+zlhb42tTTv+ixuQDO87wr7cA9nswjqcoV80oV80oV80EY66O1tqokz3hlaI+G8aYNdbaRKdznEi5aka5aka5aqa25dKhDxERP6eiFhHxc/5Y1LOdDnAKylUzylUzylUztSqX3x2jFhGRf+ePe9QiInIcFbWIiJ9zrKiNMVcaY7KMMdnGmMkneb6uMebv1c+vNsbE+EmuO4wx+caY9dUf432Q6U1jTJ4xJu0UzxtjzAvVmVOMMX29ncnNXBcbYwqOW1dTfZSrgzHmC2NMhjEm3Rhzz0mW8fk6czOXz9eZMaaeMeYHY8yG6lyPnGQZn2+Pbuby+fZ43HuHGGN+MsYsPslznl1f1lqff1A1LnUL0AkIBzYAPU5Y5rfAq9WPbwT+7ie57gD+6uP1dSHQF0g7xfMjgCWAAc4DVvtJrouBxQ58f7UB+lY/bgRsOsnX0efrzM1cPl9n1eugYfXjMGA1cN4JyzixPbqTy+fb43Hv/Ufg/ZN9vTy9vpzao/75hrnW2jLgXzfMPd41wDvVj+cBlxrv30jRnVw+Z639Cjj4C4tcA7xrq3wPNDHGtPGDXI6w1uZaa9dVPy4CNlJ178/j+XyduZnL56rXwZHqT8OqP048y8Dn26ObuRxhjGkPjAReP8UiHl1fThX1yW6Ye+I37M/LWGsrgAKguR/kAhhb/ePyPGNMh5M872vu5nbCoOofXZcYY3r6+s2rf+TsQ9Xe2PEcXWe/kAscWGfVP8avB/KAz621p1xfPtwe3ckFzmyPzwETAdcpnvfo+tIvE2vuYyDGWpsAfM7//6sp/2kdVfMLegMvAgt8+ebGmIZAEnCvtbbQl+/9S06Ty5F1Zq2ttNaeS9U9UQcYY3r54n1Px41cPt8ejTFXAXnW2rXefq9/caqo3blh7s/LGGNCgcbAAadzWWsPWGuPVX/6OtDPy5nc4Zc3ILbWFv7rR1dbdZegMGNMC1+8tzEmjKoyfM9am3ySRRxZZ6fL5eQ6q37Pw8AXwJUnPOXE9njaXA5tj0OAq40x26k6PDrUGDP3hGU8ur6cKrKBVWIAAAE/SURBVGp3bpi7CLi9+vF1wApbfWTeyVwnHMe8mqrjjE5bBNxWfSbDeUCBtTbX6VDGmNb/Oi5njBlA1feb1zfu6vd8A9horX3mFIv5fJ25k8uJdWaMiTLGNKl+XB8YBmSesJjPt0d3cjmxPVpr77fWtrfWxlDVESustb86YTGPri+37pnoafYUN8w1xjwKrLHWLqLqG/pvxphsqn5hdaOf5PqDMeZqoKI61x3ezmWM+YCqswFaGGNygIep+sUK1tpXqbqf5QggGygB7vR2JjdzXQfcbYypAI4CN/rgH1uo2uO5FUitPr4JMAWIPi6bE+vMnVxOrLM2wDvGmBCq/mH4yFq72Ont0c1cPt8eT8Wb60uXkIuI+Dn9MlFExM+pqEVE/JyKWkTEz6moRUT8nIpaRMTPqahFRPycilpExM/9H9Iv/WtMuEOqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10. What function is dy/dx based on the plot? Verify further by running this code.\n",
        "x.grad == 2 * x\n"
      ],
      "metadata": {
        "id": "7aoq5-daFpzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265a0374-4f26-4450-8945-6b51e99779b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd in NN Training\n",
        "\n",
        "We've had a brief look at how autograd works, but how does it look when it's used for its intended purpose? Let's define a small model and examine how it changes after a single training batch. First, define a few constants, our model, and some stand-ins for inputs and outputs:"
      ],
      "metadata": {
        "id": "V-_KBvmeBBc9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfUBrxziqMtL"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "DIM_IN = 1000\n",
        "HIDDEN_SIZE = 100\n",
        "DIM_OUT = 10\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "        \n",
        "        self.layer1 = torch.nn.Linear(1000, 100)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer2 = torch.nn.Linear(100, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "    \n",
        "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
        "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
        "\n",
        "model = TinyModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(some_input)\n",
        "print(ideal_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOZhNPjBAzOb",
        "outputId": "0dc34b71-7659-4a1d-fb96-58104ff063ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.9304, -1.2457,  1.0339,  ..., -0.1160, -1.0352,  0.9398],\n",
            "        [-0.2501, -0.6918, -0.9016,  ...,  0.9595, -2.0047,  2.0555],\n",
            "        [ 0.4270,  0.3835,  0.3447,  ..., -0.2208,  1.2392, -1.2151],\n",
            "        ...,\n",
            "        [-1.9973, -0.1665, -0.4906,  ...,  0.6130,  1.2908, -0.1482],\n",
            "        [-0.2585, -0.9406, -0.9890,  ..., -0.9782, -0.3580,  0.8241],\n",
            "        [ 0.9202, -0.3793, -0.4637,  ..., -0.9916, -0.9418,  0.2865]])\n",
            "tensor([[-1.4091e+00, -7.5004e-01,  2.4802e+00, -1.3603e+00, -7.4882e-01,\n",
            "         -8.9735e-01, -5.8766e-01, -1.1903e+00,  8.2312e-01,  5.0230e-01],\n",
            "        [ 2.6934e-01,  9.1505e-01, -1.5358e+00,  1.6749e+00, -1.3865e+00,\n",
            "          4.9982e-01, -1.4667e-01, -4.5258e-01, -4.9520e-02, -9.7466e-02],\n",
            "        [-1.4456e-01, -1.9154e+00,  1.0807e+00,  8.8458e-01, -1.7586e+00,\n",
            "          4.2036e-01, -5.3473e-01,  3.5452e-01, -7.2274e-01, -2.1599e-01],\n",
            "        [-2.8546e+00,  1.1351e+00,  6.7895e-01,  8.2301e-01,  3.9248e-01,\n",
            "          1.6994e+00, -4.7911e-01,  1.8219e+00,  5.8586e-02,  2.3522e-03],\n",
            "        [ 2.2513e+00,  6.7947e-01,  2.2405e-01, -9.4694e-01,  1.4052e+00,\n",
            "          5.7542e-01, -1.0364e+00, -1.8172e-01, -1.3031e+00,  1.1978e+00],\n",
            "        [-8.1216e-01, -1.6060e+00,  7.8324e-01, -1.3685e+00, -2.2531e-01,\n",
            "         -6.0203e-02, -1.2599e+00, -1.6385e-01,  1.6047e+00, -1.4102e+00],\n",
            "        [ 2.9052e-01,  8.2609e-01,  3.0763e-01,  1.4147e-01, -6.4718e-01,\n",
            "          5.2299e-01, -2.0341e-01,  6.0137e-01,  1.0781e-01, -1.2864e+00],\n",
            "        [-1.3580e+00, -3.3753e-01, -2.4213e-01, -8.6895e-01, -2.4719e+00,\n",
            "          5.1496e-01,  2.2522e-01, -8.4060e-01, -1.4734e+00,  1.1075e+00],\n",
            "        [-1.1207e-01, -1.3387e-01, -1.8725e-01,  1.0793e+00, -1.5355e+00,\n",
            "         -1.6894e+00,  6.9500e-02,  1.3836e+00, -2.7891e+00,  1.6826e+00],\n",
            "        [ 4.6764e-01,  1.0194e+00, -4.6298e-01,  4.2222e-01,  7.5299e-01,\n",
            "         -1.0295e-01,  3.1007e-01, -2.4005e-02, -8.6948e-01, -4.8351e-01],\n",
            "        [-5.7661e-01, -1.2906e+00,  4.7799e-02,  2.9183e-01, -9.9175e-01,\n",
            "          6.9091e-01, -1.3648e+00,  5.3734e-01,  5.5712e-01,  6.2509e-01],\n",
            "        [-2.0082e-02, -5.9165e-01,  9.9523e-01, -4.6563e-01, -4.9683e-01,\n",
            "          2.6781e-02, -1.2922e-01,  7.1849e-02, -1.0557e+00, -1.3048e-01],\n",
            "        [ 1.6810e+00,  2.3304e-01, -2.0879e-02, -4.8551e-01,  1.0291e+00,\n",
            "         -9.2209e-01, -8.2585e-01,  2.5243e-01,  6.5697e-01,  1.2643e-01],\n",
            "        [ 1.8608e-01,  6.4571e-01,  6.7849e-01, -6.6999e-01,  1.2323e+00,\n",
            "          9.6265e-02, -1.4248e+00,  1.0883e+00,  1.2268e+00, -4.3064e-01],\n",
            "        [ 3.5354e-01,  9.2491e-01,  1.7902e-01, -3.3079e-01, -1.0334e+00,\n",
            "         -1.3727e-01,  5.5676e-01,  6.1123e-01, -4.2482e-01, -2.2397e-01],\n",
            "        [ 9.7690e-01,  1.7821e+00,  8.1526e-01,  6.3315e-01,  2.4833e-01,\n",
            "         -3.2767e-01, -1.2170e+00, -6.1685e-01, -1.8706e-01,  1.2011e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11. How many input units does this NN have?\n",
        "# 1000 units.\n",
        "\n",
        "#Q12. How many output units does it have?\n",
        "# 10 units.\n",
        "\n",
        "#Q13. How many hidden layers does it have?\n",
        "# Just one hidden layer"
      ],
      "metadata": {
        "id": "B3hP10jARt4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTeLRPAkqMtM"
      },
      "source": [
        "One thing you might notice is that we never specify `requires_grad=True` for the model's layers. Within a subclass of `torch.nn.module`, it's assumed that we want to track gradients on the layers' weights for learning.\n",
        "\n",
        "## Initial Parameter Values\n",
        "If we look at the layers of the model, we can examine the values of the weights:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layer2.weight[0]\n",
        "# what does 'selectBackward' mean?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d135osu1BEBG",
        "outputId": "9b6cf9ef-a828-419b-d968-a7da8bf7471e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0715,  0.0673,  0.0757, -0.0967,  0.0454, -0.0573,  0.0704,  0.0432,\n",
              "         0.0215,  0.0288, -0.0530,  0.0752, -0.0002, -0.0286, -0.0693,  0.0931,\n",
              "        -0.0310, -0.0959,  0.0447, -0.0525,  0.0368, -0.0994, -0.0540,  0.0139,\n",
              "         0.0445,  0.0082, -0.0560, -0.0362, -0.0729, -0.0066, -0.0837, -0.0324,\n",
              "         0.0452, -0.0279,  0.0118,  0.0659,  0.0225,  0.0604,  0.0234, -0.0543,\n",
              "         0.0475, -0.0197, -0.0974, -0.0250,  0.0828,  0.0413, -0.0682,  0.0486,\n",
              "        -0.0131, -0.0216,  0.0246, -0.0758,  0.0495, -0.0192,  0.0297,  0.0628,\n",
              "        -0.0365, -0.0618,  0.0607,  0.0314,  0.0016,  0.0549, -0.0123,  0.0110,\n",
              "         0.0104, -0.0503,  0.0722,  0.0415,  0.0057, -0.0764, -0.0507, -0.0384,\n",
              "         0.0824, -0.0812, -0.0141, -0.0595, -0.0101,  0.0177,  0.0653, -0.0148,\n",
              "        -0.0606,  0.0904,  0.0922,  0.0556,  0.0125,  0.0033,  0.0809, -0.0464,\n",
              "        -0.0467, -0.0682,  0.0046, -0.0879,  0.0765,  0.0975, -0.0523, -0.0471,\n",
              "        -0.0569, -0.0546,  0.0038, -0.0865], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QJMgfuhqMtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d411ba3d-2276-438c-a7f7-6f9dce042d2c"
      },
      "source": [
        "print(f\"Layer2 sample weights = \\n{model.layer2.weight[0][0:10]}\") # just a small slice\n",
        "print(f\"Layer2 sample bias = \\n{model.layer2.bias[0]}\") # just one bias\n",
        "\n",
        "print(\"\\nGradients:\")\n",
        "print(f\"Weights = {model.layer2.weight.grad}\")\n",
        "print(f\"Bias = {model.layer2.bias.grad}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer2 sample weights = \n",
            "tensor([ 0.0715,  0.0673,  0.0757, -0.0967,  0.0454, -0.0573,  0.0704,  0.0432,\n",
            "         0.0215,  0.0288], grad_fn=<SliceBackward0>)\n",
            "Layer2 sample bias = \n",
            "-0.011381745338439941\n",
            "\n",
            "Gradients:\n",
            "Weights = None\n",
            "Bias = None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14. Why are the gradients 'None' for the sample weights and bias?\n",
        "#A14. Because we haven't call .backward() method yet."
      ],
      "metadata": {
        "id": "2Ip8k9ltRgmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElVD-ne-qMtM"
      },
      "source": [
        "## Forward Pass & Loss Calculation\n",
        "Let's see how this changes when we run through one training batch. For a loss function, we'll just use the square of the Euclidean distance between our `prediction` and the `ideal_output` (MSE), and we'll use a basic stochastic gradient descent optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMSVIC3mqMtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03831504-585d-4f55-9520-9c47874d59de"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "prediction = model(some_input)\n",
        "\n",
        "loss = (ideal_output - prediction).pow(2).sum() # MSE\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(152.8475, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUrIEJD_qMtM"
      },
      "source": [
        "## Backpropagating the Loss: `backward()`\n",
        "Now, let's call `loss.backward()` and see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZxtaspRqMtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a39160-cca9-4765-dad9-37a9affd5e82"
      },
      "source": [
        "loss.backward()\n",
        "# Print the parameters\n",
        "print(\"Parameters:\")\n",
        "print(f\"Layer2 sample weights = {model.layer2.weight[0][0:10]}\")\n",
        "print(f\"Layer2 sample bias = {model.layer2.bias[0]}\")\n",
        "\n",
        "# Print the gradients\n",
        "print(\"\\nGradients:\")\n",
        "print(f\"Layer2 sample weight gradients = {model.layer2.weight.grad[0][0:10]}\")\n",
        "print(f\"Layer2 sample bias gradient = {model.layer2.bias.grad[0]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters:\n",
            "Layer2 sample weights = tensor([ 0.0715,  0.0673,  0.0757, -0.0967,  0.0454, -0.0573,  0.0704,  0.0432,\n",
            "         0.0215,  0.0288], grad_fn=<SliceBackward0>)\n",
            "Layer2 sample bias = -0.011381745338439941\n",
            "\n",
            "Gradients:\n",
            "Layer2 sample weight gradients = tensor([-0.7557, -1.8322,  6.0227,  2.5740, -1.9469,  2.9355,  1.8538, -1.3650,\n",
            "        -2.2657, -2.5952])\n",
            "Layer2 sample bias gradient = 1.2641019821166992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15. Are the sample weights and bias the same as before the loss is backpropagated?\n",
        "#A15. Yes, the weights are the same because, we do not train even one loop, just did backpropagation.  \n",
        "\n",
        "#Q16. Are the gradients of the sample weights and bias the same as before the loss is backpropagated?\n",
        "#A16. No, previously, it did not have any gradient but now we have gradient by calling loss.backward()."
      ],
      "metadata": {
        "id": "MeUKCLt2SnNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asSdpVIvqMtN"
      },
      "source": [
        "## Updating the Parameters with the Optimiser: `step()`\n",
        "We can see that the gradients have been computed for each learning weight, but the weights remain unchanged, because we haven't run the optimiser yet. The optimiser is responsible for updating model weights based on the computed gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzETNdd2qMtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056c88b2-2bd6-46d8-a399-c3874c1e17c2"
      },
      "source": [
        "optimizer.step() # 한번의 loop를 돌린 개념\n",
        "\n",
        "print(\"Parameter updates:\")\n",
        "print(f\"Layer2 weights = {model.layer2.weight[0][0:10]}\")\n",
        "print(f\"Layer2 bias = {model.layer2.bias[0]}\")\n",
        "\n",
        "print(\"\\nGradients:\")\n",
        "print(f\"Layer2 weight gradients = {model.layer2.weight.grad[0][0:10]}\")\n",
        "print(f\"Layer2 bias gradients = {model.layer2.bias.grad[0]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter updates:\n",
            "Layer2 weights = tensor([ 0.0723,  0.0691,  0.0697, -0.0992,  0.0473, -0.0602,  0.0685,  0.0446,\n",
            "         0.0238,  0.0314], grad_fn=<SliceBackward0>)\n",
            "Layer2 bias = -0.012645847164094448\n",
            "\n",
            "Gradients:\n",
            "Layer2 weight gradients = tensor([-0.7557, -1.8322,  6.0227,  2.5740, -1.9469,  2.9355,  1.8538, -1.3650,\n",
            "        -2.2657, -2.5952])\n",
            "Layer2 bias gradients = 1.2641019821166992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtrGaKX0qMtN"
      },
      "source": [
        "You should see that `layer2`'s sample weights and bias have changed.\n",
        "\n",
        "## Resetting the gradients: `zero_grad()`\n",
        "One important thing about the process: After calling `optimizer.step()`, you need to call `optimizer.zero_grad()`, or else every time you run `loss.backward()`, the gradients on the learning weights will accumulate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZBDqMcPqMtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0c51db-bf40-4aa5-e37b-bcc2d41e259d"
      },
      "source": [
        "print(f\"Layer2 some weights: \\n{model.layer2.weight[0][0:10]}\")\n",
        "print(f\"Layer2 one bias: {model.layer2.bias[0]}\")\n",
        "\n",
        "for i in range(0, 5):\n",
        "    prediction = model(some_input)\n",
        "    loss = (ideal_output - prediction).pow(2).sum()\n",
        "    loss.backward()\n",
        "    \n",
        "print(\"\\nAfter a few iterations of training:\")\n",
        "print(f\"Layer2 some weights GRADs: \\n{model.layer2.weight.grad[0][0:10]}\")\n",
        "print(f\"Layer2 one bias GRAD: {model.layer2.bias.grad[0]}\")\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "print(\"\\nAfter resetting gradients:\")\n",
        "print(f\"Layer2 some weights = {model.layer2.weight.grad[0][0:10]}\")\n",
        "print(f\"Layer2 one bias = {model.layer2.bias.grad[0]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer2 some weights: \n",
            "tensor([-0.0604, -0.0863,  0.0163, -0.0865, -0.0021,  0.0691, -0.0485, -0.0251,\n",
            "         0.0605, -0.0264], grad_fn=<SliceBackward0>)\n",
            "Layer2 one bias: -0.011091786436736584\n",
            "\n",
            "After a few iterations of training:\n",
            "Layer2 some weights GRADs: \n",
            "tensor([  0.0553,   0.9123, -12.7409,   5.8534, -15.0571,  -0.9653,  -4.8379,\n",
            "         -7.7049,  -1.8245, -36.6950])\n",
            "Layer2 one bias GRAD: -51.45423889160156\n",
            "\n",
            "After resetting gradients:\n",
            "Layer2 some weights = tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Layer2 one bias = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17. Why are the gradients after running loss.backward() multiple times much bigger?\n",
        "#A17. Because we did not put optimizer.zero_grad() method.\n"
      ],
      "metadata": {
        "id": "y-ShDsdCYdv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDDjrsTFqMtN"
      },
      "source": [
        "## Turning Autograd Off and On\n",
        "\n",
        "There are situations where you will need fine-grained control over whether autograd is enabled. There are multiple ways to do this, depending on the situation.\n",
        "\n",
        "The simplest is to change the `requires_grad` flag on a tensor directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYrJ9XR0qMtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a61400e7-0f33-498b-c9a9-e1daef578d31"
      },
      "source": [
        "a = torch.ones(2, 3, requires_grad=True)\n",
        "print(f'a = {a}')\n",
        "\n",
        "b1 = 2 * a\n",
        "print(f'b1 = {b1}')\n",
        "\n",
        "a.requires_grad = False\n",
        "b2 = 2 * a\n",
        "print(f'b2 = {b2}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], requires_grad=True)\n",
            "b1 = tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
            "b2 = tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4Yd1rLTqMtO"
      },
      "source": [
        "In the cell above, we see that `b1` has a `grad_fn` (i.e., a traced computation history), which is what we expect, since it was derived from a tensor, `a`, that had autograd turned on. When we turn off autograd explicitly with `a.requires_grad = False`, computation history is no longer tracked, as we see when we compute `b2`.\n",
        "\n",
        "If you only need autograd turned off temporarily, a better way is to use the `torch.no_grad()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "75yoOI_jqMtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3950a44b-ac2f-401e-f4df-0339d7e17ffc"
      },
      "source": [
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = a + b\n",
        "print(f\"c1: \\n{c1}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    c2 = a + b\n",
        "\n",
        "print(f\"c2: \\n{c2}\")\n",
        "\n",
        "c3 = a * b\n",
        "print(f\"c3: \\n{c3}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1: \n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "c2: \n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "c3: \n",
            "tensor([[6., 6., 6.],\n",
            "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18. Can we do differentiation on computations on c1? Why?\n",
        "#A18. Yes, since both a and b are tensor with 'requires_grad=True'.\n",
        "\n",
        "#Q19. Can we do differentiation on computations on c2? Why?\n",
        "#A19. No. C2 is computed with 'torch.no_grad()'.\n",
        "\n",
        "#Q20. Can we do differentiation on computations on c3? Why?\n",
        "#A19. Yes, since only c2 is done without tracking history temporarily."
      ],
      "metadata": {
        "id": "wSmDXbuObJ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zSspgZVqMtO"
      },
      "source": [
        "`torch.no_grad()` can also be used as a function or method dectorator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YD1Vt0aqMtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cba1044-94cc-408d-dba8-ff83ab131d51"
      },
      "source": [
        "def add_tensors1(x, y):\n",
        "    return x + y\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_tensors2(x, y):\n",
        "    return x + y\n",
        "\n",
        "\n",
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = add_tensors1(a, b)\n",
        "print(c1)\n",
        "\n",
        "c2 = add_tensors2(a, b)\n",
        "print(c2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yni7EgQlqMtO"
      },
      "source": [
        "There's a corresponding context manager, `torch.enable_grad()`, for turning autograd on when it isn't already. It may also be used as a decorator.\n",
        "\n",
        "Finally, you may have a tensor that requires gradient tracking, but you want a copy that does not. For this we have the `Tensor` object's `detach()` method - it creates a copy of the tensor that is *detached* from the computation history:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIcn3CdOqMtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf01f4d-01ad-42a9-fe09-a6d50d55e22d"
      },
      "source": [
        "x = torch.rand(5, requires_grad=True)\n",
        "y = x.detach()\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3789, 0.4885, 0.1580, 0.3413, 0.2825], requires_grad=True)\n",
            "tensor([0.3789, 0.4885, 0.1580, 0.3413, 0.2825])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHZ3uAX9qMtP"
      },
      "source": [
        "We did this above when we wanted to graph some of our tensors. This is because `matplotlib` expects a NumPy array as input, and the implicit conversion from a PyTorch tensor to a NumPy array is not enabled for tensors with requires_grad=True. Making a detached copy lets us move forward.\n"
      ]
    }
  ]
}