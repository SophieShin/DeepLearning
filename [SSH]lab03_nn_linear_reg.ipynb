{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[SSH]lab03-nn-linear-reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophieShin/DeepLearning/blob/main/%5BSSH%5Dlab03_nn_linear_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture Practice(Self Study)"
      ],
      "metadata": {
        "id": "D9V76L8e5CwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "nHm6T8QIyDZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([0.8,  1.3, 2.5, 2.0])\n",
        "weight = np.array([2.5, 1.7, -0.5, 1.0])\n",
        "bias = 2\n",
        "\n",
        "output = np.dot(weight, inputs) + bias\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tYx-3YayHQe",
        "outputId": "cf390744-fb75-4b10-9fec-2d25b9d658ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2])\n",
        "b = torch.tensor([3,2])\n",
        "torch.dot(a,b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sy74-bUyzJS",
        "outputId": "91410c86-2d32-4a2f-bff9-aca44f009130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2*1 + 3*1\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJz6GLX92WxJ",
        "outputId": "3349bd09-c350-4a14-926f-0ff40d4b23b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b= 2*(-1) + 3*1\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-gB3WGO2dgi",
        "outputId": "0ceab876-752e-4f82-ac7c-fce8f7b9f13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c= 5*2 + 1*(-1)\n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woico-Sl2lH9",
        "outputId": "d76cd9a3-8f8c-4e07-cb3e-9d602fda2786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Pytorch\n",
        "import trace\n",
        "\n",
        "# import vision datasets, architectures and transform tools\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Neural network functions\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#optimizer functions\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "IvNH02215B8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpLdkiC81rw_"
      },
      "source": [
        "# Lab 3 – First Neural Network\n",
        "- Work on saved a copy on your Drive, name your file appropriately, e.g. `GN-lab03.ipynb` (Korean students, please put the initial of your **family name first** because that's how you appear in the register, e.g. Kim Gyeong Min should be `KGM`)\n",
        "- Run ALL the code cells and answer the questions (code or text) before submitting the notebook. Code cell output should be visible in your submission.\n",
        "- Save your notebook (`Ctrl+S`) before downloading and submitting on iCampus. ONLY ipynb files will be accepted.\n",
        "\n",
        "## A. Implementing a basic \"forward\" pass\n",
        "Try to visualise what each neural network model looks like in terms of neurons, weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "IQT81dJfhGOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWvkX4ap2WfO"
      },
      "source": [
        "## The Perceptron (Single Neuron)\n",
        "- Every non-input NEURON has a bias, not every weight, so the bias is added for a single output neuron\n",
        "- 1 output neuron with 3 inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq_kLhOr_QqD"
      },
      "source": [
        "# I try to type all the lines by myself, so I won't run this cell, instead I will do all the line below!\n",
        "\n",
        "# Initialise some values\n",
        "input = [0.8, 1.3, 2.5]\n",
        "weights = [2.5, 1.7, -0.5]\n",
        "bias = 2\n",
        "\n",
        "# Get the weigthed sum of the inputs and add the bias\n",
        "output = input[0]*weights[0] + input[1]*weights[1] + input[2]*weights[2] + bias\n",
        "print(output)\n",
        "\n",
        "# Add a step function; output is 1 if it is 0 or more, 0 otherwise\n",
        "output = 1 if output >= 0 else 0\n",
        "\n",
        "# Print perceptron's final output\n",
        "print(f'Final perceptron output = {output}') \n",
        "\n",
        "# TODO\n",
        "# Q1. Think of a ML problem or task where a neural network would have a final output to be 0 or 1.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = [0.8, 1.3, 2.5]\n",
        "weights = [2.5, 1.7, -0.5]\n",
        "bias = 2\n",
        "\n",
        "output = input[0]*weights[0] + input[1]*weights[1] + input[2]*weights[2] + bias\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glJbLG1thOIB",
        "outputId": "f5a24761-3069-48d0-dd6f-1d72dad36f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a step function\n",
        "output = 1 if output >=0 else 0\n",
        "print(f'Final perceptron output = {output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z_GW5Owhhdz",
        "outputId": "02166f69-e248-4435-b357-5bc6452bb9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final perceptron output = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Q1. Think of a ML problem or task where a neural network would have a final output to be 0 or 1.\n",
        "\n",
        "# A1. Researcher would get the results consisting only 0 and 1. It would be suitable for binary classification,\n",
        "# however, it the problem is regression, then it would be not appropriate to interprete the result since the estimated value should be continuous."
      ],
      "metadata": {
        "id": "H6KC56rxhsdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TODO\n",
        "###### Q1. Think of a ML problem or task where a neural network would have a final output to be 0 or 1.\n",
        "\n",
        "###### A1. Researcher would get the results consisting only 0 and 1. It would be suitable for binary classification, however, it the problem is regression, then it would be not appropriate to interprete the result since the estimated value should be continuous."
      ],
      "metadata": {
        "id": "rI7wsMcKkRct"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-UJYm5MTsZ"
      },
      "source": [
        "### Example 2: Multiple neurons\n",
        "- Each neuron will have its own WEIGHT associated to each input\n",
        "- As before, each NEURON will have its own BIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxqlna_eMqpj"
      },
      "source": [
        "# I try to type all the lines by myself, so I won't run this cell, instead I will do all the line below!\n",
        "\n",
        "\n",
        "#inputs = [0.8, 1.3, 2.5, 2.0] # A sample of 4 features, e.g. temperature, humidity, wind, pressure\n",
        "\n",
        "input = [0.8, 1.3, 2.5, 2.0]\n",
        "\n",
        "weights1 = [2.5, 1.7, -0.5, 1.0]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "\n",
        "bias1 = 2\n",
        "bias2 = 3\n",
        "bias3 = 0.5\n",
        "\n",
        "output = [input[0]*weights1[0] + input[1]*weights1[1] + input[2]*weights1[2] + input[3]*weights1[3] + bias1,\n",
        "          input[0]*weights2[0] + input[1]*weights2[1] + input[2]*weights2[2] + input[3]*weights2[3] + bias2,\n",
        "          input[0]*weights3[0] + input[1]*weights3[1] + input[2]*weights3[2] + input[3]*weights3[3] + bias3]\n",
        "\n",
        "print(output)\n",
        "\n",
        "# TODO\n",
        "# Q2. How many output neurons does this network contain?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = [0.8, 1.3, 2.5, 2.0]\n",
        "\n",
        "weights1 = [2.5, 1.7, -0.5, 1.0]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "\n",
        "bias = [2, 3, 0.5]\n",
        "\n",
        "output = [input[0]*weights1[0] + input[1]*weights1[1] + input[2]*weights1[2] + input[3]*weights1[3] + bias1,\n",
        "          input[0]*weights2[0] + input[1]*weights2[1] + input[2]*weights2[2] + input[3]*weights2[3] + bias2,\n",
        "          input[0]*weights3[0] + input[1]*weights3[1] + input[2]*weights3[2] + input[3]*weights3[3] + bias3]\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf4xPYT2i1dp",
        "outputId": "4bab5c1f-899c-447f-ce04-6e439e9bd37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.96, 1.867, 2.106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### TODO\n",
        "##### Q2. How many output neurons does this network contain?\n",
        "##### A2. The network contains 3 output neurons. "
      ],
      "metadata": {
        "id": "LNpDwiS2j-rQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9KdBdw1PzAO"
      },
      "source": [
        "### Use Loops and Matrices!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDrkbsgzQtPZ"
      },
      "source": [
        "# I try to type all the lines by myself, so I won't run this cell, instead I will do all the line below!\n",
        "# This is actually what I was trying to do in the previous cells. I ended up failing to successfully writing codes.\n",
        "# This is really helpful and I learned many grammar! Thanks a lot!!\n",
        "\n",
        "inputs = [0.8, 1.3, 2.5, 2.0]\n",
        "\n",
        "weights = [[2.5, 1.7, -0.5, 1.0], \n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "layer_outputs = [] # Outputs of current layer\n",
        "for weight, bias in zip(weights, biases):\n",
        "    output = 0 # Reset current neuron output to 0\n",
        "    for input, w in zip(inputs, weight):\n",
        "        output += input*w\n",
        "    output += bias\n",
        "    layer_outputs.append(output)\n",
        "\n",
        "print(zip(weights, biases))\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [0.8, 1.3, 2.5, 2.0]\n",
        "\n",
        "weights = [[2.5, 1.7, -0.5, 1.0], \n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "layer_outputs = []\n",
        "\n",
        "for weight, bias in zip(weights, biases): # if there are more than 2 groups that I want run iteratively, then it should be fori, j in zip(A,B): for k, i in zip(C,i)\n",
        "  output = 0\n",
        "  for input, w in zip(inputs, weight):\n",
        "    output += input*w\n",
        "  output += bias\n",
        "  layer_outputs.append(output)\n",
        "\n",
        "print(zip(weights, biases))  # it shows the type of the object\n",
        "print(layer_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj1FQN0RkbWh",
        "outputId": "35df6cd4-6af4-4f2c-c019-23ddff502974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<zip object at 0x7f381230f4b0>\n",
            "[6.96, 1.867, 2.106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to make sure if I understood correctly\n",
        "\n",
        "output_practice = 0\n",
        "for i, w in zip(inputs, [2.5, 1.7, -0.5, 1.0]):\n",
        "  output_practice += i*w\n",
        "\n",
        "print(output_practice + biases[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc9sx2w1mBEy",
        "outputId": "b2ad31cc-8028-4fbc-9705-b1560b9684b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikyggUJxXtPr"
      },
      "source": [
        "## Let's use NumPy & Dot Products!\n",
        "- Multiply elements wise the corresponding values in two arrays and add them up\n",
        "- Dot product between two vectors results in a single scalar value\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJekSFlcXwM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98261c92-adb2-450c-f91e-a3abe67b7b02"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = np.array([0.8, 1.3, 2.5, 2.0])\n",
        "weights = np.array([2.5, 1.7, -0.5, 1.0])\n",
        "bias = 2\n",
        "\n",
        "output = np.dot(weights, inputs) + bias  # array*array = element wise multiplication, so in this case, use either np.dot or @\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI09vq3snLaL",
        "outputId": "06d48755-55b9-4d4d-c6fa-4f501067cd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8, 1.3, 2.5, 2. ])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.T*weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmZTLbTBnNUr",
        "outputId": "fdd0042d-1f4c-4d54-c887-30af464e398e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.  ,  2.21, -1.25,  2.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs @ weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2xrD33vnSA2",
        "outputId": "5691d0b5-ed2f-483b-9ca6-60e7f328e1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.96"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(weights, inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8_JhAHOnlLw",
        "outputId": "52763004-e1e7-4a10-af45-090785d61dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.96"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(weights, inputs) == inputs @ weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nknhW_dinpI7",
        "outputId": "75fc6f4e-69f5-44e2-d3cf-09baad7697d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2L-U4yRaCXu"
      },
      "source": [
        "When performing a dot product between a vector (input) and a matrix (weights)\n",
        "  - Weights have to be first parameter of ``np.dot()``, otherwise an error will be thrown\n",
        "  - Column size of ``weights`` = row size of ``inputs``\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9AoCO93aL6B"
      },
      "source": [
        "# I try to type all the lines by myself, so I won't run this cell, instead I will do all the line below!\n",
        "\n",
        "\n",
        "inputs = np.array([0.8, 1.3, 2.5, 2.0])\n",
        "\n",
        "weights = np.array([[2.5, 1.7, -0.5, 1.0], \n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]])\n",
        "\n",
        "biases = np.array([2, 3, 0.5])\n",
        "\n",
        "output = np.dot(weights, inputs) + biases # dot product of a vector and a matrix\n",
        "\n",
        "print(output)\n",
        "\n",
        "# TODO\n",
        "# Q3. What else can we do to perform the dot product between the inputs and weights?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([0.8, 1.3, 2.5, 2.0])\n",
        "\n",
        "weights = np.array([ [2.5, 1.7, -0.5, 1.0],\n",
        "                     [0.5, -0.91, 0.26, -0.5],\n",
        "                     [-0.26, -0.27, 0.17, 0.87]                    \n",
        "                                                ])\n",
        "\n",
        "biases = np.array([2,3,0.5])\n",
        "\n",
        "output = np.dot(weights, inputs) + biases # elements of each row should be the same\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf0Wd2Vqoh6b",
        "outputId": "e57c71f6-f1d1-4d1f-9f8c-4071bcea62ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.96  1.867 2.106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Q3. What else can we do to perform the dot product between the inputs and weights?\n",
        "\n",
        "# A3. @ can be used to do inner product calculation\n",
        "\n",
        "output2 = (weights @ inputs) + biases\n",
        "print(output2)\n",
        "print(output == output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9-fNEqnpZGu",
        "outputId": "b433f038-14f7-4088-ac77-ba3f3c30bf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.96  1.867 2.106]\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTxtXoYEb0ea"
      },
      "source": [
        "\n",
        "## Summary\n",
        "- Weights and biases are two different tools for approximating something (e.g. an output from an input)\n",
        "- Weights determine how much influence this input has for the output (as it is multiplied) and bias determines how much this output can be offset by the input (as it is added)\n",
        "- Think of a classical equation for a line ``y = wx + b`` where ``w`` is the weight and ``b`` is a the bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiPxh-XmJqPT"
      },
      "source": [
        "## Dot product between two 2-D arrays (two matrices)\n",
        "- Numpy's ``np.dot()`` is flexible; it computes the inner product for 1D arrays and performs matrix multiplication for 2D arrays.\n",
        "- Since both matrices have the same size, transpose the second matrix, i.e. ``weights`` here so that ``1st-col-size=2nd-row-size`` holds \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhGET9UWKoIL"
      },
      "source": [
        "# I try to type all the lines by myself, so I won't run this cell, instead I will do all the line below!\n",
        "\n",
        "# inputs 3x4\n",
        "inputs = np.array([[0.8, 1.3, 2.5, 2.0],\n",
        "          [2.3, 1.5, 3.8, 4.5],\n",
        "          [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# weights 3x4\n",
        "weights = np.array([[2.5, 1.7, -0.5, 1.0], \n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]])\n",
        "\n",
        "biases = np.array([2, 3, 0.5])\n",
        "\n",
        "output = np.dot(inputs, weights.T) + biases\n",
        "\n",
        "print(f'{output} \\n{output.shape}')\n",
        "\n",
        "# TODO\n",
        "# Q4. Explain what it means to have a 2-D array for the input and also for the output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([[0.8, 1.3, 2.5, 2.0],\n",
        "          [2.3, 1.5, 3.8, 4.5],\n",
        "          [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "weights = np.array([[2.5, 1.7, -0.5, 1.0], \n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]])\n",
        "\n",
        "biases = np.array([2, 3, 0.5])\n",
        "\n",
        "weights.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkyLtYtLq9hK",
        "outputId": "47efdb61-28eb-4afb-e167-294d276b5683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.5 ,  0.5 , -0.26],\n",
              "       [ 1.7 , -0.91, -0.27],\n",
              "       [-0.5 ,  0.26,  0.17],\n",
              "       [ 1.  , -0.5 ,  0.87]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output = np.dot(inputs, weights.T) + biases\n",
        "\n",
        "print(f'{output} \\n{output.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVVbQX9UrXlv",
        "outputId": "203181ad-8f17-4064-f07c-1d8759ee3881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6.96   1.867  2.106]\n",
            " [12.9    1.523  4.058]\n",
            " [ 0.39   1.051  0.026]] \n",
            "(3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs @ weights.T + biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUb5eaB8rHS9",
        "outputId": "52fc42a3-a0ae-4298-9ca8-46cdc6f5d6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6.96 ,  1.867,  2.106],\n",
              "       [12.9  ,  1.523,  4.058],\n",
              "       [ 0.39 ,  1.051,  0.026]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.dot(inputs[0], weights[0]))\n",
        "print(inputs[0] @ weights[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIxzLVy6rdGJ",
        "outputId": "9bc3d2e2-3e21-44d2-a558-c597d2773f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.96\n",
            "4.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CwJvbURr68m",
        "outputId": "71050fcd-e0a8-47e4-aab6-6a9264272b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8, 1.3, 2.5, 2. ])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OCTF7eBr8Ws",
        "outputId": "3c0c91fc-3ebc-4ec3-c166-ca83a2041068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  1.7, -0.5,  1. ])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The below is what I found from the error.\n",
        "# row-wise, no need to transpose, but in matrixs' multiplication, then the size should be matched!\n",
        "print(np.dot(inputs[0], weights.T[0]))\n",
        "print(inputs[0] @ weights.T[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "SX2atiyIriqH",
        "outputId": "26ea32a9-0224-4581-eb3f-2dc08a4cdc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-2fa97aa65129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(np.dot(inputs[0], weights.T[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### TODO\n",
        "###### Q4. Explain what it means to have a 2-D array for the input and also for the output\n",
        "##### A4. if input and output are 2-D array(Matrix), then it means that putting bunch of data at once and getting the result at the same time.if input is 1-D array, it means running the model with one sample data."
      ],
      "metadata": {
        "id": "jivutD2F2cy1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJCjLVN4ZrL8"
      },
      "source": [
        "## Let's use Tensors!\n",
        "- ``torch.dot()`` behaves differently to ``np.dot()`` \n",
        "- ``torch.dot(a,b)`` treats both ``a`` and ``b`` as 1D vectors (irrespective of their original shape) and computes their inner product.\n",
        "- Several ways to do matrix (rank 2 tensor) multiplication:\n",
        "  - ``ab = a.mm(b)``\n",
        "  - ``ab = torch.mm(a, b)``\n",
        "  - ``ab = torch.matmul(a, b)``\n",
        "  - ``ab = a @ b # Python 3.5+``\n",
        "- The number of columns of the first matrix must be equal to the number of rows of the second matrix.\n",
        "- The function ``torch.matmul()`` performs matrix multiplications if both arguments are 2D and computes their dot product if both arguments are 1D.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qlNIDiPxSrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460c90a9-3cd1-44a7-9c42-b52df9945469"
      },
      "source": [
        "# 1-D tensor dot product\n",
        "a = torch.tensor([1, 2])\n",
        "b = torch.tensor([3, 2])\n",
        "\n",
        "torch.dot(a, b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1,2])\n",
        "b = torch.tensor([3,2])\n",
        "\n",
        "torch.dot(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7FXLPVds5n7",
        "outputId": "a97d34ea-49cf-4df9-9578-7b3e4ecee156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puj-YybH0E6g"
      },
      "source": [
        "# 2-D tensor dot product for matrices with matching column and row size\n",
        "a = torch.tensor([\n",
        "    [0, 1, 2],\n",
        "    [1, 2, 3]\n",
        "])\n",
        "\n",
        "b = torch.tensor([\n",
        "    [0, 1],\n",
        "    [1, 1],\n",
        "    [1, 3]\n",
        "])\n",
        "\n",
        "torch.mm(a,b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([\n",
        "                  [0,1,2],\n",
        "                  [1,2,3]\n",
        "])\n",
        "\n",
        "b = torch.tensor([\n",
        "                  [0,1],\n",
        "                  [1,1],\n",
        "                  [1,3]\n",
        "])\n",
        "\n",
        "print(torch.mm(a,b))\n",
        "print('\\n')\n",
        "print(a.mm(b))\n",
        "print('\\n')\n",
        "print(torch.matmul(a,b))\n",
        "print('\\n')\n",
        "print(a @ b)\n",
        "\n",
        "# all works well"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgmAWgpjtJ7O",
        "outputId": "96f6bb88-a04f-4cb5-9d94-157b09df91e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3,  7],\n",
            "        [ 5, 12]])\n",
            "\n",
            "\n",
            "tensor([[ 3,  7],\n",
            "        [ 5, 12]])\n",
            "\n",
            "\n",
            "tensor([[ 3,  7],\n",
            "        [ 5, 12]])\n",
            "\n",
            "\n",
            "tensor([[ 3,  7],\n",
            "        [ 5, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwHbk4uPA6gw"
      },
      "source": [
        "### Dot product between two 2-D tensors from the example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoK1kTu_Zu7K"
      },
      "source": [
        "input = torch.tensor(inputs)\n",
        "weight = torch.tensor(weights)\n",
        "bias = torch.tensor(biases)\n",
        "\n",
        "output = torch.matmul(input, torch.transpose(weight, 0, 1)) + bias\n",
        "\n",
        "print(output, output.shape)\n",
        "\n",
        "# TODO\n",
        "# Q5. repeat using the @ operator. You should get the same output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU0trRUHuPh_",
        "outputId": "2b5ce24c-ddd1-4ccd-80a0-a84372e5f9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.8,  1.3,  2.5,  2. ],\n",
              "       [ 2.3,  1.5,  3.8,  4.5],\n",
              "       [-1.5,  2.7,  3.3, -0.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXbtOrU0uSIL",
        "outputId": "58ca5d4d-f9ec-4576-d5a1-99c565cd64ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.5 ,  1.7 , -0.5 ,  1.  ],\n",
              "       [ 0.5 , -0.91,  0.26, -0.5 ],\n",
              "       [-0.26, -0.27,  0.17,  0.87]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM6MhFPouVcj",
        "outputId": "89e5af2a-07a4-43ce-ba1b-6befb875c96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2. , 3. , 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(inputs)\n",
        "weight = torch.tensor(weights)\n",
        "bias = torch.tensor(biases)\n",
        "\n",
        "print(input, '\\n', weight, '\\n', bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73xnDoF2uXS6",
        "outputId": "7e85db4b-e319-46e1-9a2d-3e9fdfad07f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8000,  1.3000,  2.5000,  2.0000],\n",
            "        [ 2.3000,  1.5000,  3.8000,  4.5000],\n",
            "        [-1.5000,  2.7000,  3.3000, -0.8000]], dtype=torch.float64) \n",
            " tensor([[ 2.5000,  1.7000, -0.5000,  1.0000],\n",
            "        [ 0.5000, -0.9100,  0.2600, -0.5000],\n",
            "        [-0.2600, -0.2700,  0.1700,  0.8700]], dtype=torch.float64) \n",
            " tensor([2.0000, 3.0000, 0.5000], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(weight,0,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfnzUzVku4K7",
        "outputId": "a2e88fb1-c7fd-4fc9-a336-386013cab04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5000,  0.5000, -0.2600],\n",
              "        [ 1.7000, -0.9100, -0.2700],\n",
              "        [-0.5000,  0.2600,  0.1700],\n",
              "        [ 1.0000, -0.5000,  0.8700]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(weight, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLx_1iGVu8cO",
        "outputId": "08009d39-bbb3-4c17-ba96-e1cf965ba224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5000,  0.5000, -0.2600],\n",
              "        [ 1.7000, -0.9100, -0.2700],\n",
              "        [-0.5000,  0.2600,  0.1700],\n",
              "        [ 1.0000, -0.5000,  0.8700]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(weight,1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9rA7-P5vA1j",
        "outputId": "554e0bb5-fa4d-4b23-97b7-d073493a2520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5000,  1.7000, -0.5000,  1.0000],\n",
              "        [ 0.5000, -0.9100,  0.2600, -0.5000],\n",
              "        [-0.2600, -0.2700,  0.1700,  0.8700]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(weight, 0, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYflSkO4vGcf",
        "outputId": "aeb5c8f3-c6b5-4834-bd5d-8d193dd75d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5000,  1.7000, -0.5000,  1.0000],\n",
              "        [ 0.5000, -0.9100,  0.2600, -0.5000],\n",
              "        [-0.2600, -0.2700,  0.1700,  0.8700]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = torch.matmul(input, torch.transpose(weight, 0, 1)) + bias\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fV8nKXQvrIU",
        "outputId": "59a07e1d-8251-4fde-b432-7c80773a0f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.9600,  1.8670,  2.1060],\n",
              "        [12.9000,  1.5230,  4.0580],\n",
              "        [ 0.3900,  1.0510,  0.0260]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = torch.mm(input, torch.transpose(weight, 0, 1)) + bias\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NZsuY06v6h2",
        "outputId": "08c75360-b924-4676-9e81-90cb5568e7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.9600,  1.8670,  2.1060],\n",
              "        [12.9000,  1.5230,  4.0580],\n",
              "        [ 0.3900,  1.0510,  0.0260]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output, output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj8lEvmrv-nn",
        "outputId": "e24fc1cd-244c-47e8-9bdf-8a28682fb801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6.9600,  1.8670,  2.1060],\n",
            "        [12.9000,  1.5230,  4.0580],\n",
            "        [ 0.3900,  1.0510,  0.0260]], dtype=torch.float64) torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Q5. repeat using the @ operator. You should get the same output\n",
        "\n",
        "output2 = input @ torch.transpose(weight, 0, 1) + bias\n",
        "output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t28oYYYIwJnM",
        "outputId": "fcebaa1e-58fd-4414-843e-31384028370d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.9600,  1.8670,  2.1060],\n",
              "        [12.9000,  1.5230,  4.0580],\n",
              "        [ 0.3900,  1.0510,  0.0260]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output == output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJn-A6mMwRRK",
        "outputId": "d600cf33-fc6e-42e9-ee08-ecd00083a01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jBRPHQKG3lt"
      },
      "source": [
        "### Exercise: Write your own code using TENSORS to implement a neural network as follows:\n",
        "- Has 5 input neurons made up of random numbers between 0 and 1\n",
        "- Connects to two output neurons\n",
        "- Weights and biases should be random numbers between a normal distribution. Use `torch.randn()`\n",
        "- The final outputs should be a combination of the weighted sum followed by an activation function as follows:\n",
        "  - If the weighted sum is negative, then the output is 0\n",
        "  - Otherwise the output is just the weighted sum\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvDjJncONJpx"
      },
      "source": [
        "# Q6. Has 5 input neurons made up of random numbers between 0 and 1. Use torch.rand()\n",
        "\n",
        "# Q7. Weights should be random numbers between a normal distribution. Use torch.randn()\n",
        "\n",
        "# Q8. Biases should be random numbers between a normal distribution. Use torch.randn()\n",
        "\n",
        "# Q9. The outputs should be a combination of the weighted sum of the inputs followed by an activation given below in 10.\n",
        "\n",
        "# Q10. If the weighted sum is negative, then the output is 0, otherwise the output is just the weighted sum\n",
        "\n",
        "# Print out the final output values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Has 5 input neurons made up of random numbers between 0 and 1. Use torch.rand()\n",
        "\n",
        "input = torch.rand(1,5)\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBzWzW-Kw9pd",
        "outputId": "f79314b9-e7b3-4f5d-ac42-60efd5d13c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1243, 0.1311, 0.7923, 0.6197, 0.4906]])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Weights should be random numbers between a normal distribution. Use torch.randn()\n",
        "\n",
        "weight = torch.randn(5,2)\n",
        "weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgjOPqpOxjF2",
        "outputId": "f0107825-3fe8-4031-a032-8218e7a8c73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8634, -0.8153],\n",
              "        [-0.5361,  0.3549],\n",
              "        [ 0.2446,  0.3223],\n",
              "        [ 0.6975, -0.1268],\n",
              "        [ 1.4454,  0.8039]])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Biases should be random numbers between a normal distribution. Use torch.randn()\n",
        "\n",
        "bias = torch.randn(1,2)\n",
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwgeAIynxyXp",
        "outputId": "70fb4d44-fead-4116-e7c7-349e508b4f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1743,  0.9499]])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9. The outputs should be a combination of the weighted sum of the inputs followed by an activation given below in 10.\n",
        "# Q10. If the weighted sum is negative, then the output is 0, otherwise the output is just the weighted sum\n",
        "# Print out the final output values\n",
        "\n",
        "output = torch.mm(input, weight) + bias\n",
        "\n",
        "print(output)\n",
        "\n",
        "output[0][0] = output[0][0] if output[0][0] >= 0 else 0\n",
        "output[0][1] = output[0][1] if output[0][1] >= 0 else 0\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBXLpN3qx6Kt",
        "outputId": "f45fe907-1b80-4db1-859f-267f8915429d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.9475, 2.3407]])\n",
            "tensor([[1.9475, 2.3407]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KvsIo2UfwO3"
      },
      "source": [
        "## B. Build NN Model using ``torch.nn.Module``\n",
        "- Create a subclass of nn.Module\n",
        "- Define layers and activations\n",
        "- Do a forward pass to inspect the model, parameters and outputs\n",
        "- Add loss, optimiser and training loop for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuZwA3o9ZWZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7c3b10-470f-4f00-ce86-cc37c4432a82"
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyModel(nn.Module): # TinyModel is a subclass of nn.Module\n",
        "\n",
        "# __init__, forward 두개 함수는 꼭 써야함!\n",
        "    \n",
        "    def __init__(self):          # initialze all the setting\n",
        "        super(TinyModel, self).__init__() # call the parent class, nn.Module to inherit all its attributes\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(100, 200) # input_size and hidden_size(input size = 100,input layer에는 200개의 bias가 있음(implicitly) next layer=200)\n",
        "        self.activation = nn.ReLU() # Rectified Linear Unit activation function\n",
        "        self.linear2 = nn.Linear(200, 10) # hidden layer and output layer size(200 nodes, 200 nodes match되어야 함! previous layer에서 받은 200개를 받음)\n",
        "        self.softmax = nn.Softmax() # output layer activation function\n",
        "    \n",
        "    def forward(self, x): # feed-forward network! # actual engine!\n",
        "        out = self.linear1(x)\n",
        "        out = self.activation(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "# Create an instance of TinyModel (like initalising a variable)\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "# See what this model contains\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "# Print out one of the layers\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "# View ALL the parameters (weights and biases)\n",
        "# print('\\n\\nModel params:')\n",
        "# for param in tinymodel.parameters():\n",
        "#     print(param)\n",
        "\n",
        "# View the parameters (weights and biases) in the LAST layer (linear2) only\n",
        "print('\\n\\nLinear2 layer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "TinyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "\n",
            "Just one layer:\n",
            "Linear(in_features=200, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Linear2 layer params:\n",
            "Parameter containing:\n",
            "tensor([[-0.0282, -0.0024,  0.0195,  ...,  0.0266, -0.0487,  0.0609],\n",
            "        [-0.0269, -0.0551, -0.0574,  ..., -0.0653, -0.0346, -0.0548],\n",
            "        [-0.0623,  0.0549,  0.0583,  ...,  0.0010,  0.0229, -0.0324],\n",
            "        ...,\n",
            "        [ 0.0167,  0.0053,  0.0473,  ...,  0.0595,  0.0012, -0.0097],\n",
            "        [-0.0386,  0.0252,  0.0435,  ...,  0.0576, -0.0004,  0.0030],\n",
            "        [ 0.0533, -0.0543,  0.0022,  ..., -0.0022,  0.0162,  0.0229]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0363, -0.0359,  0.0303, -0.0081, -0.0650,  0.0446, -0.0517, -0.0634,\n",
            "        -0.0559, -0.0686], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel,self).__init__()\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(100,200)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(200,10)\n",
        "    self.softmax = nn.Softmax()   # why self.softmax? self.activation does not work?\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.activation(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.softmax(out)\n",
        "    return output"
      ],
      "metadata": {
        "id": "vVuUZAV54cCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of TinyModel (like initalising a variable)\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "# See what this model contains\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "# Print out one of the layers\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)"
      ],
      "metadata": {
        "id": "n5ahGjpA5OSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = MyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(mymodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii-RyZ125TZF",
        "outputId": "1d5f60a1-1094-4242-d215-c476af8c62e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model:\n",
            "MyModel(\n",
            "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Just one layer')\n",
        "print(mymodel.linear2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXPF5Znb5Z7e",
        "outputId": "6cda3e74-0738-4e9b-c2b1-908d25e40e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just one layer\n",
            "Linear(in_features=200, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcjYoJIAAD8-"
      },
      "source": [
        "### Linear layer demo\n",
        "\n",
        "For the code chunk below, note that:\n",
        "- the weight, bias and output tensor dimensions are as expected (weight matrix is in transposed form, i.e. has shape (out_features, in_features)\n",
        "- the multiplication of the inputs with the weight matrices, added with the bias will produce the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6RhRO_CZWZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607c34dc-9f46-464b-bfb8-1062cee96d28"
      },
      "source": [
        "# Create a linear layer with 3 inputs and 2 outputs\n",
        "lin = torch.nn.Linear(3, 2)\n",
        "# Create sample input vector with random values\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "# Print the initial parameter values (these are also randomly set)\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "# Apply the linear layer to the input\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[0.2965, 0.2533, 0.0754]])\n",
            "\n",
            "\n",
            "Weight and Bias parameters:\n",
            "Parameter containing:\n",
            "tensor([[-0.3755, -0.1249,  0.4751],\n",
            "        [-0.4161, -0.3371, -0.4501]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.4087, -0.5365], requires_grad=True)\n",
            "\n",
            "\n",
            "Output:\n",
            "tensor([[-0.5159, -0.7792]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3,2)\n",
        "lin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgTpF-Fc5xOs",
        "outputId": "41264630-b2e0-4093-f812-0d7bc3b99a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=3, out_features=2, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1,3)\n",
        "print('Input:')\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vowEvWWX5-Pe",
        "outputId": "ab5b3473-8417-4ac9-a65c-acfe3689369f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "tensor([[0.9179, 0.9430, 0.0390]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Weight and Bias parameter:')\n",
        "\n",
        "for param in lin.parameters():\n",
        "  print(param)\n",
        "\n",
        "# it shows the (random) weights and biases."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqaRs1bl6GR-",
        "outputId": "2019ba89-7606-42e9-d276-6b9d6206b3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight and Bias parameter:\n",
            "Parameter containing:\n",
            "tensor([[-0.3127,  0.3719,  0.1113],\n",
            "        [-0.0686,  0.5441,  0.4242]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.5721,  0.4126], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLEuNlCr6a9e",
        "outputId": "536543f0-e93d-49ce-e61b-f7a354ebcf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Linear(in_features=3, out_features=2, bias=True)>"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psnfIWn16xhw",
        "outputId": "7657c715-8776-48bb-fa35-9094ae42c9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9179, 0.9430, 0.0390]])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin"
      ],
      "metadata": {
        "id": "1aR9TGC3640p",
        "outputId": "286159e5-fc95-4eda-97fe-4a808257c683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=3, out_features=2, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = lin(x)\n",
        "print('Output:')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8IRYPGy6v2r",
        "outputId": "ba2769c8-62d4-4af0-9419-a9061af75986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "tensor([[-0.5041,  0.8793]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rX90VHvC0kZ"
      },
      "source": [
        "- Note that you see ``requires_grad`` for the parameters. This means that their values will be tracked and used for **gradient** calculation during the training phase.\n",
        "- Although ``Parameter`` is a subclass of ``Tensor``, it has a special extended behaviour in that its gradients can be tracked for NN learning purposes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSMHg7NXqUcp"
      },
      "source": [
        "### Add Training Loop, Loss and Optimiser\n",
        "- A loop is necessary to allow NN training to happen iteratively.\n",
        "- The number of times we want to repeat the cycle is called number of epochs.\n",
        "- Loss functions tell us how far a model's prediction is from the correct answer. PyTorch contains a variety of loss functions, including common MSE (mean squared error = L2 norm), Cross Entropy Loss and Negative Likelihood Loss (useful for classifiers), and others.\n",
        "- An optimiser helps to minimise the loss by taking iterative steps in the direction that minimises the loss by updating the parameters in each epoch. We will use Stochastic Gradient Descent (SGD) for this example.\n",
        "\n",
        "\n",
        "## Example: Linear Regression\n",
        "1. Provide training data. We will provide the inputs, ``x`` and outputs, ``y`` for the formula ``y = 3x + 2``. \n",
        "  - Remember to provide them as tensors. \n",
        "  - Note that ``y`` will have to be correct values that satisfy the formula.\n",
        "2. Define the NN class. \n",
        "  - Since we will only take in a number and also spit out a number, we will have the ``input_size`` to be 1 and the ``output_size`` to be 1 as well. \n",
        "  - For linear regression, the output will be a real value, so no output layer activation function is needed.\n",
        "  - Note also that an extra parameter ``hidden_size`` was added to the initialiser\n",
        "3. Create an instance of the NN class\n",
        "  - Pass in the required parameter ``hidden_size``\n",
        "4. Define Loss and Optimiser\n",
        "  - for this linear regression problem, we will use the MSE Loss and SGD optimiser\n",
        "5. Training Loop\n",
        "  - Loop through a set number of times, e.g. 100\n",
        "    - forward pass to calculate the predicted values for each input sample and calculate the loss using the loss instance you created earlier\n",
        "    - backward pass to first reset all the gradients ``opt.zero_grad()``, then to calculate the gradients of the loss w.r.t. the params ``loss.backward()`` and finally update the params accordingly ``opt.step()``\n",
        "\n",
        "6. Test the model\n",
        "  - At this point, the model has been trained, i.e. the parameters have been optimised and the loss should be small enough.\n",
        "  - Call ``model()`` with any new number to test what the trained model will predict. See how accurately it performs.\n",
        "  - Test with several other numbers.\n",
        "\n",
        "### NOTES:\n",
        "- Data will need to be prepared so that it can be used with PyTorch. Here we have specified the data as a tensor and also made sure that it is of type float. \n",
        "- It is good practice to specify hyper parameters separately, e.g. ``hidden_size``, ``learning_rate``, ``num_epochs``, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdiTwoHS93b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f160d632-84fa-4ebc-e3a0-a0d9fe168c69"
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# 1. Define training data for a simple mathematical formula y = 3x + 2 for x=1, ..., 5\n",
        "x = torch.tensor([[1],[2],[3],[4],[5]], dtype=torch.float32)\n",
        "y = torch.tensor([[5],[8],[11],[14],[17]], dtype=torch.float32)\n",
        "\n",
        "# 2. Define NN class \n",
        "class MyNN(nn.Module): # TinyModel is a subclass of nn.Module\n",
        "    \n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_size) # input_size and hidden_size\n",
        "        self.activation = nn.ReLU() # Rectified Linear Unit activation function\n",
        "        self.linear2 = nn.Linear(hidden_size, 1) # hidden layer and output layer size\n",
        "        # output layer is a real value for regression, does not need any activation\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# 3. Create an instance of NN model and define the hidden_size\n",
        "hidden_size = 4\n",
        "model = MyNN(hidden_size) # now 'model' will be a callable function\n",
        "\n",
        "# 4. Loss and Optimiser\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.MSELoss() # now 'loss_fn' will be a callable function\n",
        "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 5. Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # 5.1 Forward pass\n",
        "  y_pred = model(x)\n",
        "  loss = loss_fn(y_pred, y) \n",
        "\n",
        "  # 5.2 Backward pass\n",
        "  opt.zero_grad() # Reset gradients at each epoch\n",
        "  loss.backward() # Backpropagate the loss gradients w.r.t the params\n",
        "  opt.step() # Update params\n",
        "\n",
        "  # 5.3 Print loss every 10th epoch\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss={loss.item():.4f} ')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss=0.0833 \n",
            "Epoch 20/100, Loss=0.0625 \n",
            "Epoch 30/100, Loss=0.0473 \n",
            "Epoch 40/100, Loss=0.0358 \n",
            "Epoch 50/100, Loss=0.0271 \n",
            "Epoch 60/100, Loss=0.0205 \n",
            "Epoch 70/100, Loss=0.0156 \n",
            "Epoch 80/100, Loss=0.0118 \n",
            "Epoch 90/100, Loss=0.0089 \n",
            "Epoch 100/100, Loss=0.0068 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "x = torch.tensor([[1],[2],[3],[4],[5]], dtype=torch.float32)\n",
        "y = torch.tensor([[5],[8],[11],[14],[17]], dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "TyFCp9zztBwy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinReg(nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(1, hidden_size)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "iyVdwLywqO1V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 4\n",
        "model = LinReg(hidden_size)\n",
        "\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "DBmYvetkq6Z1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  y_pred = model(x)\n",
        "  loss = loss_fn(y_pred, y)\n",
        "\n",
        "  opt.zero_grad()  # need to memorize these 3 lines. # should be updated not reset at each epoch??\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  if (epoch+1) % 10 == 0 : # every 10 step\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Loss = {loss.item(): .4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w2P_P4Eraol",
        "outputId": "baf9944f-ffbd-42bd-958a-f27a069feacd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss =  0.1814\n",
            "Epoch 20/100, Loss =  0.1363\n",
            "Epoch 30/100, Loss =  0.1029\n",
            "Epoch 40/100, Loss =  0.0779\n",
            "Epoch 50/100, Loss =  0.0591\n",
            "Epoch 60/100, Loss =  0.0450\n",
            "Epoch 70/100, Loss =  0.0342\n",
            "Epoch 80/100, Loss =  0.0261\n",
            "Epoch 90/100, Loss =  0.0199\n",
            "Epoch 100/100, Loss =  0.0152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_num = 10\n",
        "test = torch.tensor([test_num], dtype = torch.float32)\n",
        "model(test).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAQn8-n9tJGn",
        "outputId": "4d6d21bc-7a14-47a7-a1af-21de8f5db42f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.3023796081543"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFkv3nJWtQIG",
        "outputId": "64aa4ce3-d6c8-40ce-af43-be4203c224a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([32.3024], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Model prediction for {test_num} is {model(test).item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT8nw2QntY71",
        "outputId": "ab19fbdf-15a8-4266-8a7f-d8490edcf09d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction for 10 is 32.3024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvmHgY1oDLiB"
      },
      "source": [
        "# TODO\n",
        "# Q11. Did the loss decrease over time during training?\n",
        "# A11. Yes, it decreases over time\n",
        "\n",
        "# Q12. Was the prediction of the model for x=10 close to the ideal value that it should have?\n",
        "# A12. The real value is 32 and the prediction of the model is 32.3024. So it is very close to the ground truth value.\n",
        "\n",
        "# Q13. Change the num_epochs to 200 and retrain the model. Is the prediction for x=10 better? Why?\n",
        "\n",
        "# Q14. Set num_epoch=100 and change the hidden_size to 8. Retrain the model. Is the model's performance better?\n",
        "\n",
        "# Q15. Try increasing or decreasing num_epoch and/or hidden_size to see which combination gives you the best model. \n",
        "# Keep the output of your code execution above for the best combination and state which combination of num_epoch \n",
        "# and hidden_size values gave you the best model.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13. Change the num_epochs to 200 and retrain the model. Is the prediction for x=10 better? Why?\n",
        "# It gives slightly better result with smaller loss. It is gradient descent method and the loss function is convex function.\n",
        "# Therefore, as the number of epochs is increasing, the weights can be closer to the optimal combination.\n",
        "\n",
        "for epoch in range(200):\n",
        "  y_pred = model(x)\n",
        "  loss = loss_fn(y_pred, y)\n",
        "\n",
        "  opt.zero_grad()  # need to memorize these 3 lines. # should be updated not reset at each epoch??\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  if (epoch+1) % 10 == 0 : # every 10 step\n",
        "      print(f'Epoch {epoch+1}/{200}, Loss = {loss.item(): .4f}')\n",
        "\n",
        "\n",
        "test_num = 10\n",
        "test = torch.tensor([test_num], dtype = torch.float32)\n",
        "print(model(test).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCJIAFDtyhd",
        "outputId": "c233dae6-27f3-4c58-b068-a23e085f721f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss =  0.0116\n",
            "Epoch 20/200, Loss =  0.0089\n",
            "Epoch 30/200, Loss =  0.0068\n",
            "Epoch 40/200, Loss =  0.0052\n",
            "Epoch 50/200, Loss =  0.0040\n",
            "Epoch 60/200, Loss =  0.0031\n",
            "Epoch 70/200, Loss =  0.0023\n",
            "Epoch 80/200, Loss =  0.0018\n",
            "Epoch 90/200, Loss =  0.0014\n",
            "Epoch 100/200, Loss =  0.0011\n",
            "Epoch 110/200, Loss =  0.0008\n",
            "Epoch 120/200, Loss =  0.0006\n",
            "Epoch 130/200, Loss =  0.0005\n",
            "Epoch 140/200, Loss =  0.0004\n",
            "Epoch 150/200, Loss =  0.0003\n",
            "Epoch 160/200, Loss =  0.0002\n",
            "Epoch 170/200, Loss =  0.0002\n",
            "Epoch 180/200, Loss =  0.0001\n",
            "Epoch 190/200, Loss =  0.0001\n",
            "Epoch 200/200, Loss =  0.0001\n",
            "32.0316276550293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14. Set num_epoch=100 and change the hidden_size to 8. Retrain the model. Is the model's performance better?\n",
        "\n",
        "# The result is worse than the original model's one. I assumed that it would be better.\n",
        "# I think it happens because the initial weights are randomized, so it can depend on the initial value\n",
        "# Another hypothesis is that if the model is way complicated than it should be, the result can be not optimal\n",
        "\n",
        "hidden_size = 8\n",
        "model2 = LinReg(hidden_size)\n",
        "\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = torch.optim.SGD(model2.parameters(), lr = learning_rate)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  y_pred = model2(x)\n",
        "  loss = loss_fn(y_pred, y)\n",
        "\n",
        "  opt.zero_grad()  # need to memorize these 3 lines. # should be updated not reset at each epoch??\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  if (epoch+1) % 10 == 0 : # every 10 step\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Loss = {loss.item(): .4f}')\n",
        "\n",
        "\n",
        "test_num = 10\n",
        "test = torch.tensor([test_num], dtype = torch.float32)\n",
        "model2(test).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U15PpXPuYdu",
        "outputId": "53d64360-3eee-4227-e62c-a362f0ed622f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss =  0.0979\n",
            "Epoch 20/100, Loss =  0.0635\n",
            "Epoch 30/100, Loss =  0.0413\n",
            "Epoch 40/100, Loss =  0.0269\n",
            "Epoch 50/100, Loss =  0.0176\n",
            "Epoch 60/100, Loss =  0.0115\n",
            "Epoch 70/100, Loss =  0.0076\n",
            "Epoch 80/100, Loss =  0.0050\n",
            "Epoch 90/100, Loss =  0.0033\n",
            "Epoch 100/100, Loss =  0.0022\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.21519088745117"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Q15. Try increasing or decreasing num_epoch and/or hidden_size to see which combination gives you the best model. \n",
        "# Keep the output of your code execution above for the best combination and state which combination of num_epoch \n",
        "# and hidden_size values gave you the best model.\n",
        "\n",
        "# If both the number of hidden_nodes and epoch are increased, the result gets better.\n",
        "\n",
        "hidden_size = 8\n",
        "model3 = LinReg(hidden_size)\n",
        "\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = torch.optim.SGD(model3.parameters(), lr = learning_rate)\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  y_pred = model3(x)\n",
        "  loss = loss_fn(y_pred, y)\n",
        "\n",
        "  opt.zero_grad()  # need to memorize these 3 lines. # should be updated not reset at each epoch??\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  if (epoch+1) % 10 == 0 : # every 10 step\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Loss = {loss.item(): .4f}')\n",
        "\n",
        "\n",
        "test_num = 10\n",
        "test = torch.tensor([test_num], dtype = torch.float32)\n",
        "model3(test).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Buj6VMwz9P",
        "outputId": "480452ff-9176-47ed-f4e7-d4597e1a07eb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/200, Loss =  0.0027\n",
            "Epoch 20/200, Loss =  0.0020\n",
            "Epoch 30/200, Loss =  0.0015\n",
            "Epoch 40/200, Loss =  0.0011\n",
            "Epoch 50/200, Loss =  0.0009\n",
            "Epoch 60/200, Loss =  0.0007\n",
            "Epoch 70/200, Loss =  0.0005\n",
            "Epoch 80/200, Loss =  0.0004\n",
            "Epoch 90/200, Loss =  0.0003\n",
            "Epoch 100/200, Loss =  0.0003\n",
            "Epoch 110/200, Loss =  0.0002\n",
            "Epoch 120/200, Loss =  0.0002\n",
            "Epoch 130/200, Loss =  0.0002\n",
            "Epoch 140/200, Loss =  0.0002\n",
            "Epoch 150/200, Loss =  0.0001\n",
            "Epoch 160/200, Loss =  0.0001\n",
            "Epoch 170/200, Loss =  0.0001\n",
            "Epoch 180/200, Loss =  0.0001\n",
            "Epoch 190/200, Loss =  0.0001\n",
            "Epoch 200/200, Loss =  0.0001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32.05524826049805"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj6pLZXIX3Q_"
      },
      "source": [
        "### Let's plot the landscape of the MSE loss over a range of values for weight\n",
        "- The sample training data has function ``y=3x+2``. However, this relationship is **NOT** known by the model. We only have ``x``'s and ``y``'s.\n",
        "- So we try different values of weight, ``w``, between -1 and 5 to fit the equation ``y = wx + 2``\n",
        "- When we do that, we calculate the MSE loss between the predicted outcome, ``pred`` and the actual outcome, ``y`` for each ``w`` that we use.\n",
        "- The loss that occurred for each weight is plotted and connected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GM_vu-urCfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b8f01b3e-f819-407b-a646-0aeba0c58ea8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a range of values from -1 to 5\n",
        "w_range = np.linspace(-1, 5, 1000)\n",
        "loss_w_range = [] # create an empty list to store the loss values\n",
        "\n",
        "# Try different weight values to get predictions for y\n",
        "for w in w_range:\n",
        "    pred = (w * x) + 2\n",
        "    loss = torch.mean((pred - y) ** 2)\n",
        "    loss_w_range.append(loss.item())\n",
        "\n",
        "# Plot the graph\n",
        "plt.title(\"Loss Over Different Weight Values for the Linear Function y=3x+2\")\n",
        "plt.plot(w_range, loss_w_range)\n",
        "plt.xlabel('Weight, w')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w_range = np.linspace(-1, 5, 1000)\n",
        "loss_w_range = []\n",
        "\n",
        "for w in w_range:\n",
        "  pred = w*x + 2\n",
        "  loss = torch.mean((pred-y)**2)\n",
        "  loss_w_range.append(loss.item())\n",
        "\n",
        "plt.title('Loss over differennt weight values for the linear function y=3x+2')\n",
        "plt.plot(w_range, loss_w_range)\n",
        "plt.xlabel('Weight, w')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ti7Hulzpxl6M",
        "outputId": "e012ea7b-6a4b-44ff-89d3-64190fee76e1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdbsu201V2lU"
      },
      "source": [
        "# Q16. From the graph, what value of weight gives the minimum loss for this linear function?\n",
        "# A16. 3 makes the loss minimum."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}