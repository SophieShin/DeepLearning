{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[AIM5004]Assignment#3(SoheeShin).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcENJDzkrIFINE+OJ6y+Mr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophieShin/DeepLearning/blob/main/%5BAIM5004%5DAssignment_3(SoheeShin).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment #3\n",
        "### Training a character-level language model with recurrent neural networks and Transformers architecture. \n"
      ],
      "metadata": {
        "id": "m-fNmDTFtPjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5WW1nTTmZGhU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from utils import  forward_tracer, backward_tracer, Char2Vec, num_flat_features\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import string\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "s-AspH3boULt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Download shakespeare dataset from https://storage.googleapis.com/download. tensorflow.org/data/shakespeare.txt. Report the number of unique characters and this number will be the number of your vocabulary (note that ’a’ and ’A’ are different characters). Also, show 3 random chunks (200 chracters per each) of the dataset."
      ],
      "metadata": {
        "id": "hna23566dbs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIlmler6biuO",
        "outputId": "74319098-6a53-4cdd-c6cd-6562a95ed115"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx1yLxCPcdtT",
        "outputId": "b40ee24a-d2b3-467e-f6d2-16672a177288"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIRpKuM2cmIb",
        "outputId": "66ae260b-5d08-4033-fdf7-918f6b8c3681"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "print ('Number of Unique Characters: {}'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdzqDa4Ncyzt",
        "outputId": "89475491-a656-4de7-faaf-3a6144ac616a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Number of Unique Characters: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0,len(text) - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return text[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCav_200cyqM",
        "outputId": "fd28925d-ba2e-457a-e67d-5ea35b10c052"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e marriage\n",
            "Her nurse is privy: and, if aught in this\n",
            "Miscarried by my fault, let my old life\n",
            "Be sacrificed, some hour before his time,\n",
            "Unto the rigour of severest law.\n",
            "\n",
            "PRINCE:\n",
            "We still have known thee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  (2 pts) Design a vanila RNN architecture and write the training codes w/ following hyperparameters. Report the number of model parameters."
      ],
      "metadata": {
        "id": "GWu0j8dndzXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "RH6aRdVsblYa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 200\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(3):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXg2BdLBbPa8",
        "outputId": "9c4f9cc9-f9b4-41e9-f6ec-559d6c70ddcf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvNTxTBfbvCz",
        "outputId": "2dd3931b-f248-40ad-fc89-c3b600997f03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
            "\"know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be\"\n",
            "' done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but'\n",
            "' the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to parti'\n",
            "'cularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecon'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "5HgKtSYcb1K8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNHQxuShb3OV",
        "outputId": "d0f3eeef-7371-4a0c-e218-075838a8a10b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
            "Target:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"{:4d}\".format(i))\n",
        "    print(\"  Input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  Expected Output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8hQVh-Nb8uc",
        "outputId": "0db5cef2-b7b4-4055-a1ca-f3152fc9eb4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0\n",
            "  Input: 18 ('F')\n",
            "  Expected Output: 47 ('i')\n",
            "   1\n",
            "  Input: 47 ('i')\n",
            "  Expected Output: 56 ('r')\n",
            "   2\n",
            "  Input: 56 ('r')\n",
            "  Expected Output: 57 ('s')\n",
            "   3\n",
            "  Input: 57 ('s')\n",
            "  Expected Output: 58 ('t')\n",
            "   4\n",
            "  Input: 58 ('t')\n",
            "  Expected Output: 1 (' ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOX_tzg4cC06",
        "outputId": "d2c1ecd5-5de1-4691-db08-52a07ce0c991"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 200), dtype=tf.int64, name=None), TensorSpec(shape=(64, 200), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 64\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "2E8hTdzQcH5L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "le8I2pOwcNCL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "fbXQpudocPiR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (Batch Size, Length of Sequence, Vocab Size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIW_Sm8scTOX",
        "outputId": "b051ffb9-5b79-488f-8ff2-72e8ab68ad70"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 200, 65) # (Batch Size, Length of Sequence, Vocab Size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALCwWlddcZ0r",
        "outputId": "fdc4d720-017c-47a8-ca10-b957cfc62be6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 64)            4160      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          4460544   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,531,329\n",
            "Trainable params: 4,531,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"expected batch(shape): \", example_batch_predictions.shape)\n",
        "print(\"Loss:          \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_lR58Yfckwu",
        "outputId": "3f9a74db-86bb-4771-8a1e-7f49becd2b48"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expected batch(shape):  (64, 200, 65)\n",
            "Loss:           4.1741786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "-wZF6Casf48Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n5OhI5af9iA",
        "outputId": "38126b5b-55a2-49ee-cf82-edaf4e61c4fe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "86/86 [==============================] - 10s 114ms/step - loss: 2.6213\n",
            "Epoch 2/30\n",
            "86/86 [==============================] - 10s 114ms/step - loss: 2.2102\n",
            "Epoch 3/30\n",
            "86/86 [==============================] - 10s 115ms/step - loss: 2.0011\n",
            "Epoch 4/30\n",
            "86/86 [==============================] - 10s 117ms/step - loss: 1.8374\n",
            "Epoch 5/30\n",
            "86/86 [==============================] - 10s 119ms/step - loss: 1.7120\n",
            "Epoch 6/30\n",
            "86/86 [==============================] - 11s 121ms/step - loss: 1.6153\n",
            "Epoch 7/30\n",
            "86/86 [==============================] - 11s 123ms/step - loss: 1.5383\n",
            "Epoch 8/30\n",
            "86/86 [==============================] - 11s 125ms/step - loss: 1.4772\n",
            "Epoch 9/30\n",
            "86/86 [==============================] - 11s 127ms/step - loss: 1.4262\n",
            "Epoch 10/30\n",
            "86/86 [==============================] - 11s 129ms/step - loss: 1.3838\n",
            "Epoch 11/30\n",
            "86/86 [==============================] - 11s 129ms/step - loss: 1.3470\n",
            "Epoch 12/30\n",
            "86/86 [==============================] - 11s 124ms/step - loss: 1.3142\n",
            "Epoch 13/30\n",
            "86/86 [==============================] - 11s 125ms/step - loss: 1.2848\n",
            "Epoch 14/30\n",
            "86/86 [==============================] - 11s 128ms/step - loss: 1.2548\n",
            "Epoch 15/30\n",
            "86/86 [==============================] - 11s 129ms/step - loss: 1.2253\n",
            "Epoch 16/30\n",
            "86/86 [==============================] - 11s 127ms/step - loss: 1.1967\n",
            "Epoch 17/30\n",
            "86/86 [==============================] - 11s 128ms/step - loss: 1.1695\n",
            "Epoch 18/30\n",
            "86/86 [==============================] - 11s 125ms/step - loss: 1.1401\n",
            "Epoch 19/30\n",
            "86/86 [==============================] - 11s 127ms/step - loss: 1.1093\n",
            "Epoch 20/30\n",
            "86/86 [==============================] - 11s 128ms/step - loss: 1.0783\n",
            "Epoch 21/30\n",
            "86/86 [==============================] - 11s 130ms/step - loss: 1.0475\n",
            "Epoch 22/30\n",
            "86/86 [==============================] - 11s 125ms/step - loss: 1.0137\n",
            "Epoch 23/30\n",
            "86/86 [==============================] - 11s 127ms/step - loss: 0.9784\n",
            "Epoch 24/30\n",
            "86/86 [==============================] - 11s 125ms/step - loss: 0.9435\n",
            "Epoch 25/30\n",
            "86/86 [==============================] - 11s 128ms/step - loss: 0.9082\n",
            "Epoch 26/30\n",
            "86/86 [==============================] - 11s 129ms/step - loss: 0.8726\n",
            "Epoch 27/30\n",
            "86/86 [==============================] - 11s 126ms/step - loss: 0.8348\n",
            "Epoch 28/30\n",
            "86/86 [==============================] - 11s 126ms/step - loss: 0.8013\n",
            "Epoch 29/30\n",
            "86/86 [==============================] - 12s 138ms/step - loss: 0.7658\n",
            "Epoch 30/30\n",
            "86/86 [==============================] - 11s 126ms/step - loss: 0.7333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "Kqelf7F1gEU2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 200\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "4RFib-j8gJyc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"We are \"))"
      ],
      "metadata": {
        "id": "1s1ZH3M5tr9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa030954-007a-44f8-9038-5f8de84e9cb4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are old fault;\n",
            "Our point is happy hand'd.\n",
            "I' Varryipan:\n",
            "You that kiss me, I'll be gone with thee:\n",
            "Nay, Warwick, cannot speak, good friar,\n",
            "I mean to-morrow in my blood.\n",
            "\n",
            "LEONTES:\n",
            "More of imman, I pray, lon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"what \"))"
      ],
      "metadata": {
        "id": "Plqe_bjFuKRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98368092-34c7-4534-dc46-8c8f738bfe98"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what may canch delly:\n",
            "'Tis but the priase is his a glory.\n",
            "Lo, is he so,\n",
            "For he did lack to pass my top again,\n",
            "For 'tis another, again\n",
            "Afon of poor of prile. Now, my hastily your fellow she:\n",
            "Great you the t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"You \"))"
      ],
      "metadata": {
        "id": "v8n3EBBTuK0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7c1169-0807-48de-8031-4fc192abc4f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You knock am I.\n",
            "\n",
            "TRANIO:\n",
            "Good friar, I know; and then you are warmed, sir:\n",
            "Have I not too much of you?\n",
            "\n",
            "PAULINA:\n",
            "Trust me, I will follow\n",
            "Our uncles of rebellion, the queen too must\n",
            "Claude here from Burgut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"I tell you, friends \"))"
      ],
      "metadata": {
        "id": "KcBpdYwPuLVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4099aa-5014-42e6-a2ac-026474f212d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I tell you, friends with his soul both\n",
            "to her.\n",
            "\n",
            "Clown:\n",
            "Please you, madam:\n",
            "I am come to rest thou without fortune.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Atay, he died is grief and pack of file.\n",
            "\n",
            "GRUMIO:\n",
            "Of all would pick\n",
            "The penture of his chambe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c)  Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. Train your network RNNs and provide a PPL curve over the course of the training."
      ],
      "metadata": {
        "id": "ry4tenkAtDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(nn.NLLLoss)"
      ],
      "metadata": {
        "id": "EG8C-EYBzyOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d)  Among GRU, LSTM, or Transformer, pick one of your favorite architecture, and design the architecture whose the number of parameters is similar to vanila RNN you implemented above. Then train and provide a PPL curve over the course of the training (in the same plots in (c)). You are free to select any hyperparameters if needed (no need to use the hyperparameters above). Report the number of model parameters. "
      ],
      "metadata": {
        "id": "LGAkCCaDtavF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "w59gvgD4g0qE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "OZnB8Hk9g3fq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiuYwVB1g5q6",
        "outputId": "7de28892-efb2-4adf-b482-568d87898fd9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 200, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Expected batch(shape): \", example_batch_predictions.shape)\n",
        "print(\"Loss:          \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9G_X3GKhAJP",
        "outputId": "7cd41085-b343-4062-ad3f-4f1791984a79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected batch(shape):  (64, 200, 65)\n",
            "Loss:           4.174212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "OfiRuThdhJSc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "sEvyzaZ8hKzl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfRYH_hihN0H",
        "outputId": "ecabd7b6-d4ca-492b-d9e1-a188098b8171"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 2.9079\n",
            "Epoch 2/30\n",
            "86/86 [==============================] - 9s 107ms/step - loss: 2.3516\n",
            "Epoch 3/30\n",
            "86/86 [==============================] - 10s 109ms/step - loss: 2.1735\n",
            "Epoch 4/30\n",
            "86/86 [==============================] - 10s 109ms/step - loss: 2.0051\n",
            "Epoch 5/30\n",
            "86/86 [==============================] - 9s 106ms/step - loss: 1.8606\n",
            "Epoch 6/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.7406\n",
            "Epoch 7/30\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 1.6436\n",
            "Epoch 8/30\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 1.5676\n",
            "Epoch 9/30\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 1.5055\n",
            "Epoch 10/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.4546\n",
            "Epoch 11/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.4127\n",
            "Epoch 12/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.3764\n",
            "Epoch 13/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.3433\n",
            "Epoch 14/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.3130\n",
            "Epoch 15/30\n",
            "86/86 [==============================] - 9s 104ms/step - loss: 1.2858\n",
            "Epoch 16/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.2590\n",
            "Epoch 17/30\n",
            "86/86 [==============================] - 9s 106ms/step - loss: 1.2330\n",
            "Epoch 18/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.2080\n",
            "Epoch 19/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.1813\n",
            "Epoch 20/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.1565\n",
            "Epoch 21/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.1299\n",
            "Epoch 22/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.1035\n",
            "Epoch 23/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.0735\n",
            "Epoch 24/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.0449\n",
            "Epoch 25/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.0138\n",
            "Epoch 26/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.9834\n",
            "Epoch 27/30\n",
            "86/86 [==============================] - 9s 104ms/step - loss: 0.9503\n",
            "Epoch 28/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.9174\n",
            "Epoch 29/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.8827\n",
            "Epoch 30/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.8486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sw-ryFGGhQQR",
        "outputId": "28258246-0ce0-4a0e-c527-faeb4d3b5c17"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "KTGfjtXohTJ5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 200\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "aOd2u9RshWJ4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Pick the best performing (lowest PPL score) model, and generate the text autoregressively given the following prompts.\n"
      ],
      "metadata": {
        "id": "W2k-1a-RtnKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"We are \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaSG18G5q1Xf",
        "outputId": "1776c2d8-de15-472b-d86f-93697c3b1a6f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are malapedly.\n",
            "One--to foolish weed inhustice, howling\n",
            "That I am put to doubt, that you have wounded him, who is\n",
            "not King of nuster your thoughts,\n",
            "Which makes her the curting fatal treasure,\n",
            "The lies worm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"what \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be0HV-wBq1Me",
        "outputId": "e484ffc3-f996-4182-9387-c9aa666b5c55"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what now I may cold for me\n",
            "to be the battle king you woo a concement,\n",
            "and, by the sea, but yield in darnest. So his goodly estate we are gone\n",
            "To prattle to mine unlew my cup drunk.\n",
            "I hope to her and jots n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"You \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBWc_EfSq1BY",
        "outputId": "6811f979-ef34-48eb-d616-1f78580a8939"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You that he return?\n",
            "Our king and undertain'st armade, and ebrthes,\n",
            "And bid me see withall purpose and revoke\n",
            "As man have matesimely took to do.\n",
            "\n",
            "HORTENSIO:\n",
            "My assailing is double!\n",
            "\n",
            "SEBASTIAN:\n",
            "He is it tea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"I tell you, friends \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUFuYiVWq02F",
        "outputId": "5bbe105f-9ea7-484d-a6b7-358dc27b179a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I tell you, friends well down.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This shall denie the wife to accomplimatoon;\n",
            "A single toothinks peace encounter dawly heaven\n",
            "But to injects night: look you then well beget, here was full alone\n",
            "Thou thrive\n"
          ]
        }
      ]
    }
  ]
}