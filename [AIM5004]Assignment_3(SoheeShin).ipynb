{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[AIM5004]Assignment#3(SoheeShin).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfgZCZafyFO4LAa5lSp9Oi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophieShin/DeepLearning/blob/main/%5BAIM5004%5DAssignment_3(SoheeShin).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment #3\n",
        "### Training a character-level language model with recurrent neural networks and Transformers architecture. \n"
      ],
      "metadata": {
        "id": "m-fNmDTFtPjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5WW1nTTmZGhU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#from utils import  forward_tracer, backward_tracer, Char2Vec, num_flat_features\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import string\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "s-AspH3boULt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Download shakespeare dataset from https://storage.googleapis.com/download. tensorflow.org/data/shakespeare.txt. Report the number of unique characters and this number will be the number of your vocabulary (note that ’a’ and ’A’ are different characters). Also, show 3 random chunks (200 chracters per each) of the dataset."
      ],
      "metadata": {
        "id": "hna23566dbs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIlmler6biuO",
        "outputId": "6fb15ef2-3e10-4d21-ce05-8e3bd6e3b15a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx1yLxCPcdtT",
        "outputId": "91184f7b-c0a6-43bc-e71b-9ed014a55c40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIRpKuM2cmIb",
        "outputId": "66ae260b-5d08-4033-fdf7-918f6b8c3681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "print ('Number of Unique Characters: {}'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdzqDa4Ncyzt",
        "outputId": "aefb1dc4-6abd-400f-fa5b-36ff237e1fef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Number of Unique Characters: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0,len(text) - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return text[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCav_200cyqM",
        "outputId": "b8b05924-10fa-45d0-9080-a0f209d91b86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f,\n",
            "Shall do and undo as him pleaseth best.\n",
            "\n",
            "RICHARD:\n",
            "Let me be Duke of Clarence, George of Gloucester;\n",
            "For Gloucester's dukedom is too ominous.\n",
            "\n",
            "WARWICK:\n",
            "Tut, that's a foolish observation:\n",
            "Richard, be \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  (2 pts) Design a vanila RNN architecture and write the training codes w/ following hyperparameters. Report the number of model parameters."
      ],
      "metadata": {
        "id": "GWu0j8dndzXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "RH6aRdVsblYa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 200\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(3):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXg2BdLBbPa8",
        "outputId": "226de9a9-99df-4b72-9076-631455bfb357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvNTxTBfbvCz",
        "outputId": "29742e7f-7dad-494d-d9f6-692e8358f90e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
            "\"know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be\"\n",
            "' done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but'\n",
            "' the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to parti'\n",
            "'cularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecon'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "5HgKtSYcb1K8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNHQxuShb3OV",
        "outputId": "c6d8ddc5-1acd-4bab-c5f2-a4662ccd7e39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
            "Target:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"{:4d}\".format(i))\n",
        "    print(\"  Input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  Expected Output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8hQVh-Nb8uc",
        "outputId": "0594a550-0b86-4afd-a5be-a17725e1452e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0\n",
            "  Input: 18 ('F')\n",
            "  Expected Output: 47 ('i')\n",
            "   1\n",
            "  Input: 47 ('i')\n",
            "  Expected Output: 56 ('r')\n",
            "   2\n",
            "  Input: 56 ('r')\n",
            "  Expected Output: 57 ('s')\n",
            "   3\n",
            "  Input: 57 ('s')\n",
            "  Expected Output: 58 ('t')\n",
            "   4\n",
            "  Input: 58 ('t')\n",
            "  Expected Output: 1 (' ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOX_tzg4cC06",
        "outputId": "feafced1-7134-445d-f0eb-35e4b528285a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 200), dtype=tf.int64, name=None), TensorSpec(shape=(64, 200), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 64\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "2E8hTdzQcH5L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                 return_sequences=True,\n",
        "                                 stateful=True,\n",
        "                                 recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                 return_sequences=True,\n",
        "                                 stateful=True,\n",
        "                                 recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                 return_sequences=True,\n",
        "                                 stateful=True,\n",
        "                                 recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)        \n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "U0suLwH1NEEy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "fbXQpudocPiR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (Batch Size, Length of Sequence, Vocab Size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIW_Sm8scTOX",
        "outputId": "670c930a-02fc-4392-e7de-8d5c8b851d94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 200, 65) # (Batch Size, Length of Sequence, Vocab Size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALCwWlddcZ0r",
        "outputId": "bd233a75-65ec-43e6-d86e-d8c5cd30df23"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 64)            4160      \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (64, None, 1024)          1115136   \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (64, None, 1024)          2098176   \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (64, None, 1024)          2098176   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,382,273\n",
            "Trainable params: 5,382,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"expected batch(shape): \", example_batch_predictions.shape)\n",
        "print(\"Loss:          \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_lR58Yfckwu",
        "outputId": "318cdce0-8552-4c6a-ec88-2eadfd9a1471"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expected batch(shape):  (64, 200, 65)\n",
            "Loss:           4.32143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "-wZF6Casf48Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n5OhI5af9iA",
        "outputId": "f9e17fb0-4801-4a1a-da61-1a8b7e7691ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "86/86 [==============================] - 48s 503ms/step - loss: 4.0286\n",
            "Epoch 2/30\n",
            "86/86 [==============================] - 44s 510ms/step - loss: 3.3281\n",
            "Epoch 3/30\n",
            "86/86 [==============================] - 44s 507ms/step - loss: 3.3236\n",
            "Epoch 4/30\n",
            "86/86 [==============================] - 44s 506ms/step - loss: 3.3203\n",
            "Epoch 5/30\n",
            "86/86 [==============================] - 43s 501ms/step - loss: 3.3186\n",
            "Epoch 6/30\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 3.1490\n",
            "Epoch 7/30\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 2.7718\n",
            "Epoch 8/30\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 2.5383\n",
            "Epoch 9/30\n",
            "86/86 [==============================] - 43s 503ms/step - loss: 2.3829\n",
            "Epoch 10/30\n",
            "86/86 [==============================] - 42s 488ms/step - loss: 2.2487\n",
            "Epoch 11/30\n",
            "86/86 [==============================] - 43s 493ms/step - loss: 2.1420\n",
            "Epoch 12/30\n",
            "86/86 [==============================] - 43s 500ms/step - loss: 2.0556\n",
            "Epoch 13/30\n",
            "86/86 [==============================] - 44s 513ms/step - loss: 1.9801\n",
            "Epoch 14/30\n",
            "86/86 [==============================] - 44s 511ms/step - loss: 1.9131\n",
            "Epoch 15/30\n",
            "86/86 [==============================] - 44s 506ms/step - loss: 1.8578\n",
            "Epoch 16/30\n",
            "86/86 [==============================] - 43s 503ms/step - loss: 1.8061\n",
            "Epoch 17/30\n",
            "86/86 [==============================] - 45s 517ms/step - loss: 1.7581\n",
            "Epoch 18/30\n",
            "86/86 [==============================] - 43s 503ms/step - loss: 1.7174\n",
            "Epoch 19/30\n",
            "86/86 [==============================] - 43s 497ms/step - loss: 1.6822\n",
            "Epoch 20/30\n",
            "86/86 [==============================] - 43s 499ms/step - loss: 1.6502\n",
            "Epoch 21/30\n",
            "86/86 [==============================] - 43s 499ms/step - loss: 1.6241\n",
            "Epoch 22/30\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 1.5986\n",
            "Epoch 23/30\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 1.5789\n",
            "Epoch 24/30\n",
            "86/86 [==============================] - 43s 496ms/step - loss: 1.5602\n",
            "Epoch 25/30\n",
            "86/86 [==============================] - 43s 497ms/step - loss: 1.5444\n",
            "Epoch 26/30\n",
            "86/86 [==============================] - 43s 496ms/step - loss: 1.5301\n",
            "Epoch 27/30\n",
            "86/86 [==============================] - 43s 495ms/step - loss: 1.5161\n",
            "Epoch 28/30\n",
            "86/86 [==============================] - 43s 494ms/step - loss: 1.5039\n",
            "Epoch 29/30\n",
            "86/86 [==============================] - 42s 490ms/step - loss: 1.4908\n",
            "Epoch 30/30\n",
            "86/86 [==============================] - 43s 500ms/step - loss: 1.4821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "Kqelf7F1gEU2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 200\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "4RFib-j8gJyc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"We are \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXOwaea2VlEZ",
        "outputId": "5adaca2b-c3ec-4522-bd53-dded09ee98b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are unsider?\n",
            "My dotten hate to them follow.\n",
            "\n",
            "BIANCA:\n",
            "Anot, fair hath by me on enture it catent.\n",
            "\n",
            "TRANIO:\n",
            "Bo, there strant notle\n",
            "Which nage their courtep, to be much the cames brought to death dut of to en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"what \"))"
      ],
      "metadata": {
        "id": "Plqe_bjFuKRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98368092-34c7-4534-dc46-8c8f738bfe98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what may canch delly:\n",
            "'Tis but the priase is his a glory.\n",
            "Lo, is he so,\n",
            "For he did lack to pass my top again,\n",
            "For 'tis another, again\n",
            "Afon of poor of prile. Now, my hastily your fellow she:\n",
            "Great you the t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"You \"))"
      ],
      "metadata": {
        "id": "v8n3EBBTuK0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7c1169-0807-48de-8031-4fc192abc4f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You knock am I.\n",
            "\n",
            "TRANIO:\n",
            "Good friar, I know; and then you are warmed, sir:\n",
            "Have I not too much of you?\n",
            "\n",
            "PAULINA:\n",
            "Trust me, I will follow\n",
            "Our uncles of rebellion, the queen too must\n",
            "Claude here from Burgut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"I tell you, friends \"))"
      ],
      "metadata": {
        "id": "KcBpdYwPuLVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4099aa-5014-42e6-a2ac-026474f212d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I tell you, friends with his soul both\n",
            "to her.\n",
            "\n",
            "Clown:\n",
            "Please you, madam:\n",
            "I am come to rest thou without fortune.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Atay, he died is grief and pack of file.\n",
            "\n",
            "GRUMIO:\n",
            "Of all would pick\n",
            "The penture of his chambe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c)  Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. Train your network RNNs and provide a PPL curve over the course of the training."
      ],
      "metadata": {
        "id": "ry4tenkAtDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(nn.NLLLoss)"
      ],
      "metadata": {
        "id": "EG8C-EYBzyOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d)  Among GRU, LSTM, or Transformer, pick one of your favorite architecture, and design the architecture whose the number of parameters is similar to vanila RNN you implemented above. Then train and provide a PPL curve over the course of the training (in the same plots in (c)). You are free to select any hyperparameters if needed (no need to use the hyperparameters above). Report the number of model parameters. "
      ],
      "metadata": {
        "id": "LGAkCCaDtavF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "w59gvgD4g0qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "OZnB8Hk9g3fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiuYwVB1g5q6",
        "outputId": "7de28892-efb2-4adf-b482-568d87898fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 200, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Expected batch(shape): \", example_batch_predictions.shape)\n",
        "print(\"Loss:          \", example_batch_loss.numpy().mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9G_X3GKhAJP",
        "outputId": "7cd41085-b343-4062-ad3f-4f1791984a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected batch(shape):  (64, 200, 65)\n",
            "Loss:           4.174212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "metadata": {
        "id": "OfiRuThdhJSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "sEvyzaZ8hKzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfRYH_hihN0H",
        "outputId": "ecabd7b6-d4ca-492b-d9e1-a188098b8171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 2.9079\n",
            "Epoch 2/30\n",
            "86/86 [==============================] - 9s 107ms/step - loss: 2.3516\n",
            "Epoch 3/30\n",
            "86/86 [==============================] - 10s 109ms/step - loss: 2.1735\n",
            "Epoch 4/30\n",
            "86/86 [==============================] - 10s 109ms/step - loss: 2.0051\n",
            "Epoch 5/30\n",
            "86/86 [==============================] - 9s 106ms/step - loss: 1.8606\n",
            "Epoch 6/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.7406\n",
            "Epoch 7/30\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 1.6436\n",
            "Epoch 8/30\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 1.5676\n",
            "Epoch 9/30\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 1.5055\n",
            "Epoch 10/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.4546\n",
            "Epoch 11/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.4127\n",
            "Epoch 12/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.3764\n",
            "Epoch 13/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.3433\n",
            "Epoch 14/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.3130\n",
            "Epoch 15/30\n",
            "86/86 [==============================] - 9s 104ms/step - loss: 1.2858\n",
            "Epoch 16/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.2590\n",
            "Epoch 17/30\n",
            "86/86 [==============================] - 9s 106ms/step - loss: 1.2330\n",
            "Epoch 18/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.2080\n",
            "Epoch 19/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.1813\n",
            "Epoch 20/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.1565\n",
            "Epoch 21/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.1299\n",
            "Epoch 22/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.1035\n",
            "Epoch 23/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.0735\n",
            "Epoch 24/30\n",
            "86/86 [==============================] - 9s 103ms/step - loss: 1.0449\n",
            "Epoch 25/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 1.0138\n",
            "Epoch 26/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.9834\n",
            "Epoch 27/30\n",
            "86/86 [==============================] - 9s 104ms/step - loss: 0.9503\n",
            "Epoch 28/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.9174\n",
            "Epoch 29/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.8827\n",
            "Epoch 30/30\n",
            "86/86 [==============================] - 9s 102ms/step - loss: 0.8486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sw-ryFGGhQQR",
        "outputId": "28258246-0ce0-4a0e-c527-faeb4d3b5c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "KTGfjtXohTJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 200\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "aOd2u9RshWJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Pick the best performing (lowest PPL score) model, and generate the text autoregressively given the following prompts.\n"
      ],
      "metadata": {
        "id": "W2k-1a-RtnKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"We are \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaSG18G5q1Xf",
        "outputId": "1776c2d8-de15-472b-d86f-93697c3b1a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are malapedly.\n",
            "One--to foolish weed inhustice, howling\n",
            "That I am put to doubt, that you have wounded him, who is\n",
            "not King of nuster your thoughts,\n",
            "Which makes her the curting fatal treasure,\n",
            "The lies worm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"what \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be0HV-wBq1Me",
        "outputId": "e484ffc3-f996-4182-9387-c9aa666b5c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what now I may cold for me\n",
            "to be the battle king you woo a concement,\n",
            "and, by the sea, but yield in darnest. So his goodly estate we are gone\n",
            "To prattle to mine unlew my cup drunk.\n",
            "I hope to her and jots n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"You \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBWc_EfSq1BY",
        "outputId": "6811f979-ef34-48eb-d616-1f78580a8939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You that he return?\n",
            "Our king and undertain'st armade, and ebrthes,\n",
            "And bid me see withall purpose and revoke\n",
            "As man have matesimely took to do.\n",
            "\n",
            "HORTENSIO:\n",
            "My assailing is double!\n",
            "\n",
            "SEBASTIAN:\n",
            "He is it tea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"I tell you, friends \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUFuYiVWq02F",
        "outputId": "5bbe105f-9ea7-484d-a6b7-358dc27b179a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I tell you, friends well down.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This shall denie the wife to accomplimatoon;\n",
            "A single toothinks peace encounter dawly heaven\n",
            "But to injects night: look you then well beget, here was full alone\n",
            "Thou thrive\n"
          ]
        }
      ]
    }
  ]
}